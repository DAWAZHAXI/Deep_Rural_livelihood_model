#!/usr/bin/env python
# coding: utf-8
"""
Out-of-Region å®åŒºéªŒè¯è®­ç»ƒè„šæœ¬ï¼ˆå®½æ¾ç‰ˆ + æ–­ç‚¹ç»­è·‘ï¼‰
================================================
åœ¨åŸ Out-of-Province è„šæœ¬åŸºç¡€ä¸Šæ”¹ä¸ºâ€œå®è§‚åŒºåŸŸâ€åˆ’åˆ†ï¼Œå¹¶å®ç°ï¼š

æ–¹æ¡ˆ 2ï¼šå®½æ¾ç‰ˆ Out-of-Regionï¼ˆç›®æ ‡å®åŒºéƒ¨åˆ†æ ·æœ¬å‚ä¸è®­ç»ƒï¼‰
-------------------------------------------------------
è®¾å®š 6 ä¸ªå®åŒºï¼šä¸œåŒ—ã€ååŒ—ã€åä¸œã€ä¸­å—ã€è¥¿å—ã€è¥¿åŒ—ã€‚
å¯¹æ¯ä¸ªå®åŒº Rï¼š

1ï¼‰å°†è¯¥å®åŒºæ‰€æœ‰æ ·æœ¬é›†åˆè®°ä¸º region_idx_allã€‚
2ï¼‰å…¶ä¸­ä¸€éƒ¨åˆ†æ ·æœ¬ä½œä¸ºçœŸæ­£æµ‹è¯•é›† test_idxï¼ˆä¸å‚ä¸è®­ç»ƒä¹Ÿä¸å‚ä¸éªŒè¯ï¼‰ï¼›
3ï¼‰å‰©ä½™æ ·æœ¬ pool_target ä¸å…¶å®ƒå®åŒºæ‰€æœ‰æ ·æœ¬ other_idx_all ä¸€èµ·ç»„æˆ
   train+val æ± ï¼Œåœ¨å…¶ä¸­éšæœºåˆ’åˆ† train / valã€‚

è¿™æ ·ï¼š
- æµ‹è¯•é›†ä»ç„¶æ˜¯â€œç›®æ ‡å®åŒºå†…éƒ¨æ²¡è§è¿‡çš„ç‚¹â€ï¼›
- ä½†æ¨¡å‹åœ¨è®­ç»ƒæ—¶å·²ç»â€œè§è¿‡è¿™ä¸ªå®åŒºçš„å¤§éƒ¨åˆ†ç»Ÿè®¡ç»“æ„â€ï¼Œ
  æ˜¯ä¸€ä¸ªæ›´å®½æ¾ã€æ›´ç°å®çš„ Out-of-Region è®¾å®šã€‚
"""

import os
import numpy as np
import pandas as pd
import geopandas as gpd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, Subset, DataLoader
from sklearn.metrics import r2_score
import rasterio
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import time

print("="*80)
print("ğŸŒ Out-of-Region å®åŒºéªŒè¯è®­ç»ƒï¼ˆå®½æ¾ç‰ˆ + æ–­ç‚¹ç»­è·‘ï¼‰")
print("="*80)

# ============================================================================
#                           ç¬¬ä¸€éƒ¨åˆ†ï¼šé…ç½®å‚æ•°
# ============================================================================

# è·¯å¾„é…ç½®
DAY_TIFS = [
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_RED_2020_90m_zscore.tif",
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_GREEN_2020_90m_zscore.tif",
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_BLUE_2020_90m_zscore.tif",
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_NIR_2020_90m_zscore.tif",
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_SWIR1_2020_90m_zscore.tif",
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_SWIR2_2020_90m_zscore.tif",
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_TEMP1_2020_90m_zscore.tif",
]
NIGHT_TIF = r"F:\Landsat_NL_Mector_90m_zscore\VIIRS_2020_90m_zscore.tif"
LABEL_SHP = r"F:\sample_2020\Sample_2020.shp"
PROVINCE_SHP = r"F:\Province_boundary\Provinces_China.shp"
TARGET_FIELDS = ["F", "F_NF", "NF_F", "NF"]

OUT_DIR = r"F:\model_outputs_2020_OUT_OF_REGION_MACRO_SOFT"
os.makedirs(OUT_DIR, exist_ok=True)

# â­ æ£€æŸ¥ç‚¹ç›®å½•
CHECKPOINT_DIR = os.path.join(OUT_DIR, "checkpoints")
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# è®­ç»ƒå‚æ•°
BEST_LR = 3e-4
BEST_WD = 1e-3
PATCH_SIZE = 64
BATCH_SIZE = 256
EPOCHS = 300
PATIENCE = 15
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_WORKERS = 0

# â­ æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡
CHECKPOINT_INTERVAL = 10  # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡

# â­ å®åŒºå®½æ¾åˆ’åˆ†å‚æ•°ï¼ˆä½ å¯ä»¥æŒ‰éœ€è¦æ”¹ï¼‰
TARGET_TEST_RATIO = 0.2   # ç›®æ ‡å®åŒºæ ·æœ¬ä¸­ç”¨äº test çš„æ¯”ä¾‹ï¼ˆä¾‹å¦‚ 0.2 = 20% ç•™ä½œçœŸæ­£æµ‹è¯•ï¼‰
GLOBAL_VAL_RATIO = 0.2    # train+val æ± ä¸­ç”¨äºéªŒè¯çš„æ¯”ä¾‹ï¼ˆä¾‹å¦‚ 0.2 = 80% è®­ç»ƒï¼Œ20% éªŒè¯ï¼‰

print(f"âœ… é…ç½®å®Œæˆ")
print(f"   è®¾å¤‡: {DEVICE}")
print(f"   è¾“å‡ºç›®å½•: {OUT_DIR}")
print(f"   æ£€æŸ¥ç‚¹ç›®å½•: {CHECKPOINT_DIR}")
print(f"   æ£€æŸ¥ç‚¹é—´éš”: æ¯{CHECKPOINT_INTERVAL}ä¸ªepoch")
print(f"   å®åŒº test æ¯”ä¾‹: {TARGET_TEST_RATIO:.2f}, train/val æ±  val æ¯”ä¾‹: {GLOBAL_VAL_RATIO:.2f}")

# ============================================================================
#                    ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®é›†ç±»ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰
# ============================================================================

class PatchPointDataset(Dataset):
    """ç‚¹æ ·æœ¬æ•°æ®é›†ï¼ˆå†…å­˜ç¼“å­˜ç‰ˆï¼‰"""
    
    def __init__(self, shp_path, day_paths, night_path, target_fields,
                 patch_size=64, mode='train', check_valid=True, 
                 max_samples=None, cache_data=True):
        super().__init__()
        
        print(f"ğŸ“‚ è¯»å–shapefile: {os.path.basename(shp_path)}")
        self.gdf = gpd.read_file(shp_path).reset_index(drop=True)
        
        if max_samples is not None and max_samples < len(self.gdf):
            print(f"âš ï¸ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªä½¿ç”¨å‰ {max_samples} ä¸ªæ ·æœ¬")
            self.gdf = self.gdf.iloc[:max_samples]
        
        self.day_paths = day_paths
        self.night_path = night_path
        self.target_fields = target_fields
        self.patch = patch_size
        self.mode = mode
        self.cache_data = cache_data
        
        print(f"ğŸ”§ æ‰“å¼€æ …æ ¼æ–‡ä»¶...")
        self.day_srcs = [rasterio.open(p) for p in self.day_paths]
        self.night_src = rasterio.open(self.night_path)
        
        self.height = self.day_srcs[0].height
        self.width = self.day_srcs[0].width
        self.transform = self.day_srcs[0].transform
        
        print(f"   å½±åƒå°ºå¯¸: {self.height} Ã— {self.width}")
        
        if check_valid:
            self.valid_idx, self.cached_patches = self._build_valid_index_and_cache()
        else:
            self.valid_idx = list(range(len(self.gdf)))
            self.cached_patches = None
            print(f"âœ… è·³è¿‡é¢„ç­›ï¼Œä½¿ç”¨æ‰€æœ‰ {len(self.gdf)} ä¸ªæ ·æœ¬")
        
        if len(self.valid_idx) == 0:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨æ ·æœ¬")
        
        if self.cache_data and self.cached_patches is not None:
            print("ğŸ’¾ æ•°æ®å·²ç¼“å­˜åˆ°å†…å­˜ï¼Œå…³é—­æ …æ ¼æ–‡ä»¶")
            for s in self.day_srcs:
                s.close()
            self.night_src.close()
            self.day_srcs = None
            self.night_src = None
    
    def _check_and_load_sample(self, idx):
        """æ£€æŸ¥æ ·æœ¬æœ‰æ•ˆæ€§å¹¶åŠ è½½æ•°æ®åˆ°å†…å­˜"""
        try:
            row = self.gdf.iloc[idx]
            geom = row.geometry
            if geom is None or geom.is_empty:
                return None
            if geom.geom_type != "Point":
                geom = geom.centroid
            x, y = geom.x, geom.y
            
            half = self.patch // 2
            first_src = self.day_srcs[0]
            r, c = rasterio.transform.rowcol(first_src.transform, x, y)
            
            if (r < half or c < half or 
                r >= first_src.height - half or 
                c >= first_src.width - half):
                return None
            
            window = rasterio.windows.Window(c - half, r - half, self.patch, self.patch)
            
            day_stack = []
            for src in self.day_srcs:
                arr = src.read(1, window=window, boundless=True, masked=True)
                if arr.mask.all() or np.isnan(arr.filled(0)).all():
                    return None
                day_stack.append(arr.filled(0).astype(np.float32))
            
            day_arr = np.stack(day_stack, axis=0)
            
            night_arr = self.night_src.read(1, window=window, boundless=True, masked=True)
            if night_arr.mask.all() or np.isnan(night_arr.filled(0)).all():
                return None
            night_arr = night_arr.filled(0).astype(np.float32)[np.newaxis, :, :]
            
            vals = [row[f] for f in self.target_fields]
            if any((v is None) or (isinstance(v, float) and np.isnan(v)) for v in vals):
                return None
            
            y = np.array(vals, dtype=np.float32)
            s = y.sum()
            if s > 1e-6:
                y = y / s
            else:
                y = np.array([1.0] + [0.0] * (len(self.target_fields) - 1), dtype=np.float32)
            
            return {
                'day': day_arr,
                'night': night_arr,
                'y': y
            }
        
        except Exception:
            return None
    
    def _build_valid_index_and_cache(self):
        """å¤šçº¿ç¨‹é¢„ç­›å¹¶ç¼“å­˜æ•°æ®åˆ°å†…å­˜"""
        print("â³ å¤šçº¿ç¨‹é¢„ç­›å¹¶ç¼“å­˜æ•°æ®åˆ°å†…å­˜...")
        
        valid_idx = []
        cached_data = {} if self.cache_data else None
        
        max_workers = min(8, os.cpu_count() or 4)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self._check_and_load_sample, idx): idx 
                      for idx in range(len(self.gdf))}
            
            for future in tqdm(as_completed(futures), 
                             total=len(futures), 
                             desc="åŠ è½½æ•°æ®"):
                idx = futures[future]
                result = future.result()
                
                if result is not None:
                    valid_idx.append(idx)
                    if self.cache_data:
                        cached_data[idx] = result
        
        valid_idx.sort()
        
        if self.cache_data and cached_data:
            sample_size = (cached_data[valid_idx[0]]['day'].nbytes + 
                          cached_data[valid_idx[0]]['night'].nbytes +
                          cached_data[valid_idx[0]]['y'].nbytes) / 1e6
            total_size = sample_size * len(valid_idx)
            print(f"âœ… æœ‰æ•ˆæ ·æœ¬: {len(valid_idx)}/{len(self.gdf)} ({len(valid_idx)/len(self.gdf)*100:.1f}%)")
            print(f"   å†…å­˜å ç”¨: {total_size:.1f} MB ({sample_size:.2f} MB/æ ·æœ¬)")
        else:
            print(f"âœ… æœ‰æ•ˆæ ·æœ¬: {len(valid_idx)}/{len(self.gdf)} ({len(valid_idx)/len(self.gdf)*100:.1f}%)")
        
        return valid_idx, cached_data
    
    def __len__(self):
        return len(self.valid_idx)
    
    def set_mode(self, mode: str):
        """åˆ‡æ¢æ¨¡å¼"""
        assert mode in ["train", "val", "test"]
        self.mode = mode
    
    def _augment_day(self, day):
        """æ•°æ®å¢å¼º"""
        if np.random.rand() < 0.5:
            delta = np.random.uniform(-0.5, 0.5)
            day = day + delta
        if np.random.rand() < 0.5:
            factor = np.random.uniform(0.75, 1.25)
            day = day * factor
        return day
    
    def __getitem__(self, idx):
        real_idx = self.valid_idx[idx]
        
        if self.cached_patches is not None:
            data = self.cached_patches[real_idx]
            day = data['day'].copy()
            night = data['night'].copy()
            y = data['y'].copy()
        else:
            row = self.gdf.iloc[real_idx]
            geom = row.geometry
            if geom.geom_type != "Point":
                geom = geom.centroid
            x, y_coord = geom.x, geom.y
            r, c = rasterio.transform.rowcol(self.transform, x, y_coord)
            
            half = self.patch // 2
            window = rasterio.windows.Window(c - half, r - half, self.patch, self.patch)
            
            day_stack = []
            for src in self.day_srcs:
                arr = src.read(1, window=window, boundless=True, masked=True)
                day_stack.append(arr.filled(0).astype(np.float32))
            day = np.stack(day_stack, axis=0)
            
            night_arr = self.night_src.read(1, window=window, boundless=True, masked=True)
            night = night_arr.filled(0).astype(np.float32)[np.newaxis, :, :]
            
            vals = []
            for f in self.target_fields:
                v = row[f]
                if v is None or (isinstance(v, float) and np.isnan(v)):
                    v = 0.0
                vals.append(float(v))
            y = np.array(vals, dtype=np.float32)
            s = y.sum()
            if s > 1e-6:
                y = y / s
            else:
                y = np.array([1.0] + [0.0] * (len(self.target_fields) - 1), dtype=np.float32)
        
        if self.mode == 'train':
            if np.random.rand() < 0.5:
                day = np.flip(day, axis=1).copy()
                night = np.flip(night, axis=1).copy()
            if np.random.rand() < 0.5:
                day = np.flip(day, axis=2).copy()
                night = np.flip(night, axis=2).copy()
            k = np.random.randint(0, 4)
            if k > 0:
                day = np.rot90(day, k, axes=(1, 2)).copy()
                night = np.rot90(night, k, axes=(1, 2)).copy()
            day = self._augment_day(day)
        
        return {
            "day": torch.from_numpy(day),
            "night": torch.from_numpy(night),
            "y": torch.from_numpy(y),
        }
    
    def close(self):
        """å…³é—­æ–‡ä»¶å¥æŸ„"""
        if self.day_srcs is not None:
            for s in self.day_srcs:
                if not s.closed:
                    s.close()
        if self.night_src is not None and not self.night_src.closed:
            self.night_src.close()

# ============================================================================
#                    ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¨¡å‹å®šä¹‰ï¼ˆä¿æŒåŸæ¥ç»“æ„ï¼‰
# ============================================================================

class PreActBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.bn1 = nn.BatchNorm2d(in_channels)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                               stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu2 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, 
                               stride=1, padding=1, bias=False)
        
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Conv2d(in_channels, out_channels, 
                                     kernel_size=1, stride=stride, bias=False)
        else:
            self.shortcut = None
    
    def forward(self, x):
        out = self.bn1(x)
        out = self.relu1(out)
        shortcut = self.shortcut(out) if self.shortcut is not None else x
        out = self.conv1(out)
        out = self.bn2(out)
        out = self.relu2(out)
        out = self.conv2(out)
        return out + shortcut


class ResNetStem(nn.Module):
    def __init__(self, in_channels, base_channels=64, num_blocks=5):
        super().__init__()
        self.conv_in = nn.Conv2d(in_channels, base_channels, kernel_size=3, 
                                stride=1, padding=1, bias=False)
        self.blocks = nn.Sequential(
            *[PreActBlock(base_channels, base_channels, stride=1) 
              for _ in range(num_blocks)]
        )
        self.bn_out = nn.BatchNorm2d(base_channels)
        self.relu_out = nn.ReLU(inplace=True)
    
    def forward(self, x):
        x = self.conv_in(x)
        x = self.blocks(x)
        x = self.bn_out(x)
        x = self.relu_out(x)
        return x


class FusionResNetDirichlet(nn.Module):
    def __init__(self, day_channels=7, night_channels=1, n_comp=4,
                 base_channels=64, day_blocks=3, night_blocks=3, shared_blocks=5):
        super().__init__()
        self.day_stem = ResNetStem(day_channels, base_channels, day_blocks)
        self.night_stem = ResNetStem(night_channels, base_channels, night_blocks)
        
        self.shared_in_conv = nn.Conv2d(2 * base_channels, base_channels, 
                                       kernel_size=3, stride=1, padding=1, bias=False)
        self.shared_blocks = nn.Sequential(
            *[PreActBlock(base_channels, base_channels, stride=1) 
              for _ in range(shared_blocks)]
        )
        self.shared_bn = nn.BatchNorm2d(base_channels)
        self.shared_relu = nn.ReLU(inplace=True)
        
        self.head_conv = nn.Conv2d(base_channels, base_channels, 
                                   kernel_size=3, stride=1, padding=1, bias=False)
        self.head_bn = nn.BatchNorm2d(base_channels)
        self.head_relu = nn.ReLU(inplace=True)
        self.head_drop = nn.Dropout2d(0.1)
        self.head_out = nn.Conv2d(base_channels, n_comp, kernel_size=1, bias=True)
    
    def forward(self, day, night):
        d = self.day_stem(day)
        n = self.night_stem(night)
        x = torch.cat([d, n], dim=1)
        x = self.shared_in_conv(x)
        x = self.shared_blocks(x)
        x = self.shared_bn(x)
        x = self.shared_relu(x)
        x = self.head_conv(x)
        x = self.head_bn(x)
        x = self.head_relu(x)
        x = self.head_drop(x)
        alpha_raw = self.head_out(x)
        alpha = F.softplus(alpha_raw) + 1.0
        return alpha


def dirichlet_nll(y_true, alpha, eps=1e-7):
    """Dirichletè´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±"""
    if y_true.dim() == 2:
        pass
    elif y_true.dim() > 2:
        B, C, H, W = y_true.shape
        yc = y_true[:, :, H // 2, W // 2]
        y_true = yc
    
    B, C, H, W = alpha.shape
    ac = alpha[:, :, H // 2, W // 2]
    
    alpha0 = ac.sum(dim=1, keepdim=True)
    logC = torch.lgamma(alpha0) - torch.lgamma(ac).sum(dim=1, keepdim=True)
    
    y_safe = torch.clamp(y_true, min=eps, max=1.0 - eps)
    logL = logC + ((ac - 1.0) * torch.log(y_safe)).sum(dim=1, keepdim=True)
    return -logL.mean()


def r2_score_numpy(y_true, y_pred):
    """è®¡ç®—RÂ²åˆ†æ•°"""
    mask = np.isfinite(y_true).all(axis=1) & np.isfinite(y_pred).all(axis=1)
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    if y_true.shape[0] < 2:
        return np.nan, [np.nan] * y_true.shape[1]
    
    r2_each = [r2_score(y_true[:, i], y_pred[:, i]) 
               for i in range(y_true.shape[1])]
    r2_mean = float(np.mean(r2_each))
    return r2_mean, r2_each

# ============================================================================
#            ç¬¬å››éƒ¨åˆ†ï¼šæŒ‰â€œå®è§‚åŒºåŸŸâ€åˆ’åˆ†æ•°æ®ï¼ˆå®½æ¾ç‰ˆ Out-of-Regionï¼‰
# ============================================================================

# å®åŒºå®šä¹‰ï¼ˆå¯ä»¥æŒ‰ä½ è‡ªå·±ä¹ æƒ¯è°ƒæ•´ï¼Œä½†è¦å’Œçœåä¸€è‡´ï¼‰
MACRO_REGION_DEF = {
    "ä¸œåŒ—": ["è¾½å®çœ", "å‰æ—çœ", "é»‘é¾™æ±Ÿçœ"],
    "ååŒ—": ["åŒ—äº¬å¸‚", "å¤©æ´¥å¸‚", "æ²³åŒ—çœ", "å±±è¥¿çœ", "å†…è’™å¤è‡ªæ²»åŒº"],
    "åä¸œ": ["ä¸Šæµ·å¸‚", "æ±Ÿè‹çœ", "æµ™æ±Ÿçœ", "å®‰å¾½çœ", "ç¦å»ºçœ", "æ±Ÿè¥¿çœ", "å±±ä¸œçœ"],
    "ä¸­å—": ["æ²³å—çœ", "æ¹–åŒ—çœ", "æ¹–å—çœ", "å¹¿ä¸œçœ", "å¹¿è¥¿å£®æ—è‡ªæ²»åŒº", "æµ·å—çœ"],
    "è¥¿å—": ["é‡åº†å¸‚", "å››å·çœ", "è´µå·çœ", "äº‘å—çœ"],
    "è¥¿åŒ—": ["é™•è¥¿çœ", "ç”˜è‚ƒçœ", "é’æµ·çœ", "å®å¤å›æ—è‡ªæ²»åŒº", "æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº"],
}


def create_macroregion_folds_soft(base_ds, province_shp,
                                  target_test_ratio=0.2,
                                  global_val_ratio=0.2):
    """
    å®è§‚åŒºåŸŸåˆ’åˆ†ï¼ˆå®½æ¾ç‰ˆ Out-of-Regionï¼‰

    å¯¹æ¯ä¸ªå®åŒº Rï¼š
    - region_idx_all: è¯¥å®åŒºæ‰€æœ‰æœ‰æ•ˆæ ·æœ¬
    - å…¶ä¸­ target_test_ratio éƒ¨åˆ†ä½œä¸º test_idxï¼ˆçœŸæ­£æµ‹è¯•é›†ï¼‰
    - å‰©ä½™ pool_target ä¸å…¶å®ƒå®åŒºæ ·æœ¬ other_idx_all åˆå¹¶æˆ train+val æ± ï¼Œ
      å†æŒ‰ global_val_ratio åˆ’åˆ†å‡º val_idxï¼Œå‰©ä½™ä¸º train_idx

    è¿”å›:
        folds: é•¿åº¦ = å®åŒºæ•°ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ dict:
            {
                "macro_region": str,
                "train_provinces": [...],
                "test_provinces": [...],
                "train_idx": np.array,
                "val_idx": np.array,
                "test_idx": np.array,
            }
        province_samples: {çœå: [dataset_idx, ...]}
    """
    print("\n" + "="*80)
    print("ğŸ—ºï¸  æŒ‰å®è§‚åŒºåŸŸåˆ’åˆ†æ•°æ®é›†ï¼ˆå®½æ¾ç‰ˆ Out-of-Regionï¼‰")
    print("="*80)
    
    print("\nğŸ“‚ è¯»å–çœç•Œæ•°æ®...")
    provinces_gdf = gpd.read_file(province_shp)
    
    prov_field = 'çœ'
    if prov_field not in provinces_gdf.columns:
        raise ValueError(f"çœç•Œshapefileä¸­æ²¡æœ‰'{prov_field}'å­—æ®µ")
    
    # æ’é™¤æ¸¯æ¾³å°
    exclude_regions = ['é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒº', 'æ¾³é—¨ç‰¹åˆ«è¡Œæ”¿åŒº', 'å°æ¹¾çœ']
    provinces_gdf = provinces_gdf[~provinces_gdf[prov_field].isin(exclude_regions)]
    
    print(f"   æœ‰æ•ˆçœä»½: {len(provinces_gdf)}")
    
    print("\nğŸ”— ä¸ºæœ‰æ•ˆæ ·æœ¬åˆ†é…çœä»½...")
    valid_gdf = base_ds.gdf.iloc[base_ds.valid_idx].copy()
    valid_gdf['dataset_idx'] = range(len(base_ds.valid_idx))
    
    print(f"   æœ‰æ•ˆæ ·æœ¬æ•°: {len(valid_gdf)}")
    
    # CRS å¯¹é½
    if valid_gdf.crs != provinces_gdf.crs:
        print("   âš ï¸ CRSä¸ä¸€è‡´ï¼Œé‡æŠ•å½±ä¸­...")
        valid_gdf = valid_gdf.to_crs(provinces_gdf.crs)
    
    samples_with_prov = gpd.sjoin(
        valid_gdf,
        provinces_gdf[[prov_field, 'geometry']],
        how='left',
        predicate='within'
    )
    
    n_unassigned = samples_with_prov[prov_field].isna().sum()
    if n_unassigned > 0:
        print(f"   âš ï¸ {n_unassigned} ä¸ªæ ·æœ¬æœªåˆ†é…åˆ°çœä»½ï¼ˆè¾¹ç•Œç‚¹ï¼‰ï¼Œå°†è¢«ç§»é™¤")
        samples_with_prov = samples_with_prov[samples_with_prov[prov_field].notna()]
    
    # ç»Ÿè®¡æ¯ä¸ªçœä»½çš„æ ·æœ¬ index
    print("\nğŸ“Š æ ·æœ¬åœ¨å„çœä»½åˆ†å¸ƒ:")
    province_samples = {}
    for prov_name in sorted(samples_with_prov[prov_field].unique()):
        prov_mask = samples_with_prov[prov_field] == prov_name
        prov_indices = samples_with_prov[prov_mask]['dataset_idx'].tolist()
        province_samples[prov_name] = prov_indices
        print(f"   {prov_name}: {len(prov_indices)} æ ·æœ¬")
    
    # æ„å»ºå®åŒº folds
    print("\nğŸ”€ åˆ›å»ºå®åŒºå®½æ¾ Out-of-Region åˆ’åˆ†...")
    folds = []
    all_provinces = set(province_samples.keys())
    
    for fold_id, (macro_name, prov_list) in enumerate(MACRO_REGION_DEF.items(), 1):
        # åªä¿ç•™å®é™…å­˜åœ¨çš„çœä»½
        test_provinces = [p for p in prov_list if p in province_samples]
        if len(test_provinces) == 0:
            print(f"\n   âš ï¸ å®åŒº {macro_name} åœ¨æ•°æ®ä¸­æ²¡æœ‰ä»»ä½•çœä»½ï¼Œè·³è¿‡")
            continue
        
        trainval_provinces = sorted(list(all_provinces - set(test_provinces)))
        
        # è¯¥å®åŒºæ‰€æœ‰æ ·æœ¬
        region_idx_all = []
        for p in test_provinces:
            region_idx_all.extend(province_samples[p])
        region_idx_all = np.array(region_idx_all, dtype=int)
        
        # å…¶ä»–æ‰€æœ‰å®åŒºæ ·æœ¬
        other_idx_all = []
        for p in trainval_provinces:
            other_idx_all.extend(province_samples[p])
        other_idx_all = np.array(other_idx_all, dtype=int)
        
        # åœ¨è¯¥å®åŒºå†…éƒ¨åˆ’åˆ†å‡º test / pool_target
        rng_region = np.random.RandomState(seed=fold_id * 100 + 7)
        rng_region.shuffle(region_idx_all)
        n_region = len(region_idx_all)
        n_test_target = max(1, int(n_region * target_test_ratio))
        test_idx = region_idx_all[:n_test_target]
        pool_target = region_idx_all[n_test_target:]
        
        # train+val æ±  = å…¶ä»–å®åŒº + ç›®æ ‡å®åŒºå‰©ä½™æ ·æœ¬
        trainval_pool = np.concatenate([other_idx_all, pool_target], axis=0)
        rng_tv = np.random.RandomState(seed=fold_id * 1000 + 13)
        rng_tv.shuffle(trainval_pool)
        
        n_tv = len(trainval_pool)
        n_val = max(1, int(n_tv * global_val_ratio))
        val_idx = trainval_pool[:n_val]
        train_idx = trainval_pool[n_val:]
        
        print(f"\n   å®åŒº {macro_name}:")
        print(f"      æµ‹è¯•çœä»½ ({len(test_provinces)}): {', '.join(test_provinces)}")
        print(f"      è®­ç»ƒ+éªŒè¯çœä»½ ({len(trainval_provinces)}): {', '.join(trainval_provinces[:10])}...")
        print(f"      ç›®æ ‡å®åŒºæ ·æœ¬æ€»æ•°: {n_region}")
        print(f"         âœ æµ‹è¯•é›†: {len(test_idx)}")
        print(f"         âœ è¿›å…¥ train+val æ± : {len(pool_target)}")
        print(f"      å…¶ä»–å®åŒºæ ·æœ¬æ•°: {len(other_idx_all)}")
        print(f"      æœ€ç»ˆ train æ ·æœ¬: {len(train_idx)}")
        print(f"      æœ€ç»ˆ val æ ·æœ¬: {len(val_idx)}")
        print(f"      æœ€ç»ˆ test æ ·æœ¬: {len(test_idx)}")
        
        folds.append({
            "fold": fold_id,
            "macro_region": macro_name,
            "train_provinces": trainval_provinces,  # è®­ç»ƒ+éªŒè¯æ¥æºçš„çœ
            "test_provinces": test_provinces,
            "train_idx": train_idx,
            "val_idx": val_idx,
            "test_idx": test_idx,
        })
    
    print("\nâœ… å®åŒºåˆ’åˆ†å®Œæˆ")
    return folds, province_samples

# ============================================================================
#               ç¬¬äº”éƒ¨åˆ†ï¼šæ£€æŸ¥ç‚¹ç®¡ç†åŠŸèƒ½ï¼ˆä¿æŒåŸæ ·ï¼‰
# ============================================================================

class CheckpointManager:
    """æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, fold_id, checkpoint_dir):
        self.fold_id = fold_id
        self.checkpoint_dir = checkpoint_dir
        self.checkpoint_path = os.path.join(
            checkpoint_dir, 
            f"fold{fold_id}_checkpoint.pth"
        )
        self.meta_path = os.path.join(
            checkpoint_dir,
            f"fold{fold_id}_meta.json"
        )
    
    def save_checkpoint(self, epoch, model, optimizer, scheduler, 
                       best_val_r2, best_state, epochs_no_improve):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_val_r2': best_val_r2,
            'best_state': best_state,  # ä¿å­˜æœ€ä¼˜æ¨¡å‹çŠ¶æ€
            'epochs_no_improve': epochs_no_improve,
            'fold_id': self.fold_id,
        }
        
        try:
            temp_path = self.checkpoint_path + '.tmp'
            torch.save(checkpoint, temp_path)
            
            if os.path.exists(self.checkpoint_path):
                os.remove(self.checkpoint_path)
            
            os.rename(temp_path, self.checkpoint_path)
            
            meta = {
                'epoch': epoch,
                'best_val_r2': float(best_val_r2),
                'epochs_no_improve': epochs_no_improve,
                'fold_id': self.fold_id,
                'timestamp': time.time(),
            }
            with open(self.meta_path, 'w') as f:
                json.dump(meta, f, indent=2)
            
            return True
        except Exception as e:
            print(f"\n   âš ï¸ æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
            return False
    
    def load_checkpoint(self, model, optimizer, scheduler):
        """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹"""
        if not os.path.exists(self.checkpoint_path):
            return None
        
        try:
            print(f"\n   ğŸ”„ å‘ç°æ£€æŸ¥ç‚¹ï¼Œæ­£åœ¨åŠ è½½...")
            checkpoint = torch.load(self.checkpoint_path, map_location=DEVICE)
            
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            start_epoch = checkpoint['epoch'] + 1
            best_val_r2 = checkpoint['best_val_r2']
            best_state = checkpoint.get('best_state', None)
            epochs_no_improve = checkpoint['epochs_no_improve']
            
            print(f"   âœ… ä» Epoch {checkpoint['epoch']} æ¢å¤")
            print(f"   æœ€ä½³éªŒè¯RÂ²: {best_val_r2:.4f}")
            print(f"   æœªæ”¹å–„è½®æ•°: {epochs_no_improve}")
            
            return {
                'start_epoch': start_epoch,
                'best_val_r2': best_val_r2,
                'best_state': best_state,
                'epochs_no_improve': epochs_no_improve,
            }
        except Exception as e:
            print(f"\n   âš ï¸ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            print(f"   å°†ä»å¤´å¼€å§‹è®­ç»ƒ")
            return None
    
    def clean(self):
        """æ¸…ç†æ£€æŸ¥ç‚¹"""
        try:
            if os.path.exists(self.checkpoint_path):
                os.remove(self.checkpoint_path)
                print(f"   ğŸ—‘ï¸  å·²æ¸…ç†æ£€æŸ¥ç‚¹: {os.path.basename(self.checkpoint_path)}")
            if os.path.exists(self.meta_path):
                os.remove(self.meta_path)
        except Exception as e:
            print(f"   âš ï¸ æ£€æŸ¥ç‚¹æ¸…ç†å¤±è´¥: {e}")

# ============================================================================
#        ç¬¬å…­éƒ¨åˆ†ï¼šå• fold è®­ç»ƒå‡½æ•°ï¼ˆåŠ å…¥å®åŒºåç§°ï¼Œä½†å…¶å®ƒé€»è¾‘ä¸å˜ï¼‰
# ============================================================================

def train_one_fold_oop(base_ds, train_idx, val_idx, test_idx, 
                       fold_id, train_provinces, test_provinces,
                       macro_region=None):
    """è®­ç»ƒå•ä¸ª foldï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼Œå®½æ¾ç‰ˆ Out-of-Regionï¼‰"""
    print(f"\n{'='*80}")
    if macro_region is not None:
        print(f"ğŸ¯ Fold {fold_id} - Out-of-Region å®åŒºè®­ç»ƒï¼ˆæµ‹è¯•å®åŒº: {macro_region}ï¼‰")
    else:
        print(f"ğŸ¯ Fold {fold_id} - Out-of-Region å®åŒºè®­ç»ƒ")
    print(f"{'='*80}")
    if macro_region is not None:
        print(f"   æµ‹è¯•å®åŒº: {macro_region}")
    print(f"   è®­ç»ƒçœä»½æ•°: {len(train_provinces)}")
    print(f"   æµ‹è¯•çœä»½æ•°: {len(test_provinces)}")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(train_idx):,}")
    print(f"   éªŒè¯æ ·æœ¬: {len(val_idx):,}")
    print(f"   æµ‹è¯•æ ·æœ¬: {len(test_idx):,}")
    
    # åˆ›å»ºæ•°æ®é›†
    train_subset = Subset(base_ds, list(train_idx))
    val_subset = Subset(base_ds, list(val_idx))
    test_subset = Subset(base_ds, list(test_idx))
    
    train_loader = DataLoader(
        train_subset, batch_size=BATCH_SIZE, shuffle=True,
        num_workers=NUM_WORKERS, pin_memory=True,
        persistent_workers=True if NUM_WORKERS > 0 else False
    )
    val_loader = DataLoader(
        val_subset, batch_size=BATCH_SIZE, shuffle=False,
        num_workers=NUM_WORKERS, pin_memory=True,
        persistent_workers=True if NUM_WORKERS > 0 else False
    )
    test_loader = DataLoader(
        test_subset, batch_size=BATCH_SIZE, shuffle=False,
        num_workers=NUM_WORKERS, pin_memory=True,
        persistent_workers=True if NUM_WORKERS > 0 else False
    )
    
    # æ¨¡å‹
    model = FusionResNetDirichlet(
        day_channels=len(DAY_TIFS), night_channels=1,
        n_comp=len(TARGET_FIELDS),
    ).to(DEVICE)
    
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=BEST_LR, weight_decay=BEST_WD
    )
    
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-7
    )
    
    # æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    checkpoint_mgr = CheckpointManager(fold_id, CHECKPOINT_DIR)
    
    # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint_info = checkpoint_mgr.load_checkpoint(model, optimizer, scheduler)
    
    if checkpoint_info is not None:
        start_epoch = checkpoint_info['start_epoch']
        best_val_r2 = checkpoint_info['best_val_r2']
        best_state = checkpoint_info['best_state']
        epochs_no_improve = checkpoint_info['epochs_no_improve']
    else:
        start_epoch = 1
        best_val_r2 = -1e9
        best_state = None
        epochs_no_improve = 0
    
    print(f"\nâ³ å¼€å§‹è®­ç»ƒï¼ˆä» Epoch {start_epoch} å¼€å§‹ï¼‰...")
    pbar = tqdm(range(start_epoch, EPOCHS + 1), desc=f"Fold{fold_id}", 
                initial=start_epoch-1, total=EPOCHS)
    
    for epoch in pbar:
        # è®­ç»ƒ
        base_ds.set_mode('train')
        model.train()
        train_loss = 0.0
        for batch in train_loader:
            day = batch["day"].to(DEVICE, non_blocking=True)
            night = batch["night"].to(DEVICE, non_blocking=True)
            y = batch["y"].to(DEVICE, non_blocking=True)
            
            optimizer.zero_grad()
            alpha = model(day, night)
            loss = dirichlet_nll(y, alpha)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss += loss.item() * day.size(0)
        
        train_loss /= len(train_loader.dataset)
        
        # éªŒè¯
        base_ds.set_mode('val')
        model.eval()
        val_loss = 0.0
        y_true_all = []
        y_pred_all = []
        with torch.no_grad():
            for batch in val_loader:
                day = batch["day"].to(DEVICE, non_blocking=True)
                night = batch["night"].to(DEVICE, non_blocking=True)
                y = batch["y"].to(DEVICE, non_blocking=True)
                
                alpha = model(day, night)
                loss = dirichlet_nll(y, alpha)
                val_loss += loss.item() * day.size(0)
                
                B, C, H, W = alpha.shape
                ac = alpha[:, :, H // 2, W // 2].cpu().numpy()
                pred_comp = ac / np.clip(ac.sum(axis=1, keepdims=True), 1e-6, None)
                
                y_np = y.cpu().numpy()
                y_true_all.append(y_np)
                y_pred_all.append(pred_comp)
        
        val_loss /= len(val_loader.dataset)
        y_true_all = np.vstack(y_true_all)
        y_pred_all = np.vstack(y_pred_all)
        val_r2_mean, _ = r2_score_numpy(y_true_all, y_pred_all)
        
        scheduler.step(val_r2_mean)
        current_lr = optimizer.param_groups[0]['lr']
        
        pbar.set_postfix({
            'loss': f'{train_loss:.3f}',
            'val_r2': f'{val_r2_mean:.3f}',
            'best': f'{best_val_r2:.3f}',
            'lr': f'{current_lr:.2e}'
        })
        
        # æ›´æ–°æœ€ä¼˜æ¨¡å‹
        if val_r2_mean > best_val_r2 + 1e-4:
            best_val_r2 = val_r2_mean
            best_state = model.state_dict()
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % CHECKPOINT_INTERVAL == 0:
            checkpoint_mgr.save_checkpoint(
                epoch, model, optimizer, scheduler,
                best_val_r2, best_state, epochs_no_improve
            )
        
        # æ—©åœ
        if epochs_no_improve >= PATIENCE:
            pbar.set_description(f"Fold{fold_id} [æ—©åœ]")
            break
    
    # è®­ç»ƒå®Œæˆåæ¸…ç†æ£€æŸ¥ç‚¹
    checkpoint_mgr.clean()
    
    # æµ‹è¯•
    if best_state is not None:
        model.load_state_dict(best_state)
    
    base_ds.set_mode('val')
    model.eval()
    y_true_all = []
    y_pred_all = []
    with torch.no_grad():
        for batch in test_loader:
            day = batch["day"].to(DEVICE, non_blocking=True)
            night = batch["night"].to(DEVICE, non_blocking=True)
            y = batch["y"].to(DEVICE, non_blocking=True)
            
            alpha = model(day, night)
            B, C, H, W = alpha.shape
            ac = alpha[:, :, H // 2, W // 2].cpu().numpy()
            pred_comp = ac / np.clip(ac.sum(axis=1, keepdims=True), 1e-6, None)
            
            y_np = y.cpu().numpy()
            y_true_all.append(y_np)
            y_pred_all.append(pred_comp)
    
    y_true_all = np.vstack(y_true_all)
    y_pred_all = np.vstack(y_pred_all)
    test_r2_mean, test_r2_each = r2_score_numpy(y_true_all, y_pred_all)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model_path = os.path.join(
        OUT_DIR,
        f"model_OOR_macro_soft_fold{fold_id}_lr{BEST_LR:g}_wd{BEST_WD:g}.pth"
    )
    torch.save(best_state, model_path)
    
    print(f"\nâœ… Fold {fold_id} å®Œæˆ:")
    print(f"   éªŒè¯RÂ²: {best_val_r2:.4f}")
    print(f"   æµ‹è¯•RÂ²: {test_r2_mean:.4f}")
    print(f"   æ¨¡å‹ä¿å­˜: {model_path}")
    
    metrics = {
        "fold": fold_id,
        "macro_region": macro_region if macro_region is not None else "",
        "train_provinces": ", ".join(train_provinces),
        "test_provinces": ", ".join(test_provinces),
        "n_train": len(train_idx),
        "n_val": len(val_idx),
        "n_test": len(test_idx),
        "val_r2_best": best_val_r2,
        "test_r2_mean": test_r2_mean,
    }
    
    for name, r2v in zip(TARGET_FIELDS, test_r2_each):
        metrics[f"test_r2_{name}"] = r2v
    
    metrics["model_path"] = model_path
    
    return metrics

# ============================================================================
#               ç¬¬ä¸ƒéƒ¨åˆ†ï¼šä¸»ç¨‹åºï¼ˆæ”¯æŒè·³è¿‡å·²å®Œæˆ foldï¼‰
# ============================================================================

def main():
    """ä¸»ç¨‹åºï¼ˆå®½æ¾ç‰ˆ Out-of-Region å®åŒºåˆ’åˆ† + æ–­ç‚¹ç»­è·‘ï¼‰"""
    print("\n" + "="*80)
    print("ğŸš€ å¼€å§‹ Out-of-Region å®åŒºéªŒè¯è®­ç»ƒï¼ˆå®½æ¾ç‰ˆï¼‰")
    print("="*80)
    
    results_csv = os.path.join(OUT_DIR, "out_of_region_macro_soft_results.csv")
    
    # å·²å®Œæˆçš„ fold
    completed_folds = []
    all_results = []
    
    if os.path.exists(results_csv):
        try:
            df_existing = pd.read_csv(results_csv)
            if 'fold' in df_existing.columns:
                completed_folds = df_existing['fold'].tolist()
            all_results = df_existing.to_dict('records')
            print(f"\nğŸ“Š å‘ç°å·²å®Œæˆçš„fold: {completed_folds}")
            print(f"   å·²åŠ è½½ {len(all_results)} ä¸ªç»“æœ")
        except Exception as e:
            print(f"\nâš ï¸ è¯»å–å·²æœ‰ç»“æœå¤±è´¥: {e}")
    
    # 1. æ„å»ºæ•°æ®é›†
    print("\nğŸ“Š æ„å»ºåŸºç¡€æ•°æ®é›†...")
    base_ds = PatchPointDataset(
        LABEL_SHP, DAY_TIFS, NIGHT_TIF, TARGET_FIELDS,
        patch_size=PATCH_SIZE, mode='val', check_valid=True, 
        max_samples=None, cache_data=True
    )
    
    # 2. å®åŒºåˆ’åˆ†ï¼ˆå®½æ¾ç‰ˆ Out-of-Regionï¼‰
    folds, province_samples = create_macroregion_folds_soft(
        base_ds, PROVINCE_SHP,
        target_test_ratio=TARGET_TEST_RATIO,
        global_val_ratio=GLOBAL_VAL_RATIO
    )
    
    # 3. é€ fold è®­ç»ƒï¼ˆè·³è¿‡å·²å®Œæˆçš„ï¼‰
    for fold_info in folds:
        fold_id = fold_info["fold"]
        macro_region = fold_info["macro_region"]
        
        if fold_id in completed_folds:
            print(f"\n{'='*80}")
            print(f"â­ï¸  Fold {fold_id} (å®åŒº: {macro_region}) å·²å®Œæˆï¼Œè·³è¿‡")
            print(f"{'='*80}")
            continue
        
        train_idx = fold_info["train_idx"]
        val_idx = fold_info["val_idx"]
        test_idx = fold_info["test_idx"]
        train_provinces = fold_info["train_provinces"]
        test_provinces = fold_info["test_provinces"]
        
        try:
            metrics = train_one_fold_oop(
                base_ds, train_idx, val_idx, test_idx,
                fold_id, train_provinces, test_provinces,
                macro_region=macro_region
            )
            
            # æ›´æ–° / è¿½åŠ ç»“æœ
            existing_fold_idx = None
            for i, result in enumerate(all_results):
                if result.get('fold', None) == fold_id:
                    existing_fold_idx = i
                    break
            
            if existing_fold_idx is not None:
                all_results[existing_fold_idx] = metrics
            else:
                all_results.append(metrics)
            
            df_results = pd.DataFrame(all_results)
            df_results.to_csv(results_csv, index=False)
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_csv}")
            
        except KeyboardInterrupt:
            print(f"\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­è®­ç»ƒï¼ˆCtrl+Cï¼‰")
            print(f"   Fold {fold_id} çš„æ£€æŸ¥ç‚¹å·²ä¿å­˜")
            print(f"   é‡æ–°è¿è¡Œè„šæœ¬å°†ä»å½“å‰è¿›åº¦ç»§ç»­")
            base_ds.close()
            return
        
        except Exception as e:
            print(f"\nâŒ Fold {fold_id} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            error_result = {
                "fold": fold_id,
                "macro_region": macro_region,
                "train_provinces": ", ".join(train_provinces),
                "test_provinces": ", ".join(test_provinces),
                "error": str(e),
            }
            all_results.append(error_result)
            
            df_results = pd.DataFrame(all_results)
            df_results.to_csv(results_csv, index=False)
            continue
    
    # 4. æœ€ç»ˆæ€»ç»“
    print("\n" + "="*80)
    print("âœ… Out-of-Region å®åŒºéªŒè¯ï¼ˆå®½æ¾ç‰ˆï¼‰å®Œæˆï¼")
    print("="*80)
    
    df_results = pd.DataFrame(all_results)
    
    if 'error' in df_results.columns:
        df_success = df_results[df_results['error'].isna()]
    else:
        df_success = df_results
    
    if not df_success.empty and 'test_r2_mean' in df_success.columns:
        print("\nğŸ“Š æ€»ç»“:")
        print(f"   å¹³å‡æµ‹è¯•RÂ²: {df_success['test_r2_mean'].mean():.4f} Â± {df_success['test_r2_mean'].std():.4f}")
        print(f"\n   å„foldç»“æœ:")
        for _, row in df_success.iterrows():
            test_provs = row['test_provinces']
            if isinstance(test_provs, str) and len(test_provs) > 60:
                test_provs_disp = test_provs[:60] + "..."
            else:
                test_provs_disp = test_provs
            mr = row.get('macro_region', '')
            print(f"      Fold{int(row['fold'])} (å®åŒº: {mr}): "
                  f"RÂ²={row['test_r2_mean']:.4f} (æµ‹è¯•çœä»½: {test_provs_disp})")
    
    print(f"\n   ç»“æœä¿å­˜: {results_csv}")
    print(f"   æ¨¡å‹ä¿å­˜: {OUT_DIR}")
    
    # æ¸…ç†ç©ºçš„æ£€æŸ¥ç‚¹ç›®å½•
    try:
        if os.path.exists(CHECKPOINT_DIR) and not os.listdir(CHECKPOINT_DIR):
            os.rmdir(CHECKPOINT_DIR)
            print(f"\nğŸ—‘ï¸  å·²æ¸…ç†ç©ºæ£€æŸ¥ç‚¹ç›®å½•")
    except:
        pass
    
    base_ds.close()


if __name__ == "__main__":
    main()
