"""
å…¨å›¾æ¨ç†è„šæœ¬ - å•GPUä¼˜åŒ–ç‰ˆï¼ˆé’ˆå¯¹RTX A5000ï¼‰
===============================================

ä¼˜åŒ–ç­–ç•¥:
1. âœ… é¢„ç­›é€‰æœ‰æ•ˆçª—å£ï¼ˆ82.5%è¿‡æ»¤ï¼‰
2. âœ… æ™ºèƒ½æ–­ç‚¹ç»­ä¼ ï¼ˆéšæ—¶ä¸­æ–­æ¢å¤ï¼‰
3. âœ… DataLoaderå¤šè¿›ç¨‹åŠ é€Ÿ
4. âœ… æ˜¾å­˜ä¼˜åŒ–ï¼ˆå……åˆ†åˆ©ç”¨24GBæ˜¾å­˜ï¼‰
5. âœ… æ··åˆç²¾åº¦æ¨ç†ï¼ˆFP16åŠ é€Ÿï¼‰

é¢„æœŸæ€§èƒ½: 8å°æ—¶ â†’ 1-1.5å°æ—¶

ä½œè€…ï¼šClaude
æ—¥æœŸï¼š2025-11
"""

# %% å¯¼å…¥åº“
import os
import sys
import glob
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast
import rasterio
from rasterio.windows import Window
from tqdm import tqdm
import warnings
import time
import json
warnings.filterwarnings('ignore')

print("=" * 80)
print("ğŸ—ºï¸  å…¨å›¾æ¨ç†è„šæœ¬ - å•GPUä¼˜åŒ–ç‰ˆï¼ˆRTX A5000ï¼‰")
print("=" * 80)

# %% ============================================================================
#                           ç¬¬ä¸€éƒ¨åˆ†ï¼šé…ç½®å‚æ•°
# ==============================================================================

print("\n[1] é…ç½®å‚æ•°...")

# ========== è·¯å¾„é…ç½® ==========
DATA_DIR = r"F:\Landsat_NL_Mector_90m_zscore"
MODEL_DIR = r"F:\model_outputs_2020_resnet_optimized"
OUT_DIR = os.path.join(MODEL_DIR, "maps_2020_optimized")
BEST_MODEL_NAME = "model_fold3_rep0_lr0.0003_wd0.001.pth"

os.makedirs(OUT_DIR, exist_ok=True)

# è¾“å…¥å½±åƒ
DAY_BANDS = ["RED", "GREEN", "BLUE", "NIR", "SWIR1", "SWIR2", "TEMP1"]
DAY_TIFS = [
    os.path.join(DATA_DIR, f"Landsat_{band}_2020_90m_zscore.tif")
    for band in DAY_BANDS
]
NIGHT_TIF = os.path.join(DATA_DIR, "VIIRS_2020_90m_zscore.tif")
BEST_MODEL_PATH = os.path.join(MODEL_DIR, BEST_MODEL_NAME)

# æ¨ç†å‚æ•°
PATCH_SIZE = 64
STEP = 32
TARGET_FIELDS = ["F", "F_NF", "NF_F", "NF"]

# ä¼˜åŒ–å‚æ•°ï¼ˆé’ˆå¯¹RTX A5000 24GBï¼‰
VALID_THRESHOLD = 0      # çª—å£æœ‰æ•ˆåƒå…ƒé˜ˆå€¼  0.001 
BATCH_SIZE = 1024          # 24GBæ˜¾å­˜å¯ä»¥ç”¨æ›´å¤§çš„batch
NUM_WORKERS = 6             # CPUæ ¸å¿ƒæ•°ï¼Œå¯è°ƒæ•´
PREFETCH_FACTOR = 2         # é¢„è¯»å–æ•°é‡
USE_AMP = True              # æ··åˆç²¾åº¦ï¼ˆFP16ï¼‰åŠ é€Ÿ
PIN_MEMORY = True           # å›ºå®šå†…å­˜åŠ é€Ÿ

# æ£€æŸ¥ç‚¹è®¾ç½®
CHECKPOINT_INTERVAL = 1000  # æ¯1000ä¸ªbatchä¿å­˜ä¸€æ¬¡
AUTO_SAVE = True            # è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹

print(f"   GPU: {torch.cuda.get_device_name(0)}")
print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
print(f"   BATCH_SIZE: {BATCH_SIZE}")
print(f"   æ··åˆç²¾åº¦: {'å¯ç”¨' if USE_AMP else 'ç¦ç”¨'}")
print(f"   CPUå·¥ä½œè¿›ç¨‹: {NUM_WORKERS}")
print(f"   è¾“å‡ºç›®å½•: {OUT_DIR}")


# %% ============================================================================
#                           ç¬¬äºŒéƒ¨åˆ†ï¼šè·¯å¾„æ£€æŸ¥
# ==============================================================================

print("\n[2] æ£€æŸ¥æ–‡ä»¶è·¯å¾„...")

if not os.path.exists(DATA_DIR):
    print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}")
    sys.exit(1)

if not os.path.exists(MODEL_DIR):
    print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {MODEL_DIR}")
    sys.exit(1)

missing_files = []
for i, path in enumerate(DAY_TIFS, 1):
    if not os.path.exists(path):
        missing_files.append(f"æ—¥é—´å½±åƒ{i}")

if not os.path.exists(NIGHT_TIF):
    missing_files.append("å¤œå…‰å½±åƒ")

if not os.path.exists(BEST_MODEL_PATH):
    missing_files.append("æ¨¡å‹æ–‡ä»¶")

if missing_files:
    print("âŒ ä»¥ä¸‹æ–‡ä»¶ä¸å­˜åœ¨:")
    for mf in missing_files:
        print(f"   {mf}")
    sys.exit(1)

print("âœ… æ‰€æœ‰æ–‡ä»¶æ£€æŸ¥é€šè¿‡")


# %% ============================================================================
#                           ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¨¡å‹å®šä¹‰
# ==============================================================================

print("\n[3] å®šä¹‰æ¨¡å‹æ¶æ„...")

class PreActBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.bn1 = nn.BatchNorm2d(in_channels)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,
                               stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu2 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                               stride=1, padding=1, bias=False)

        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Conv2d(in_channels, out_channels,
                                     kernel_size=1, stride=stride, bias=False)
        else:
            self.shortcut = None

    def forward(self, x):
        out = self.bn1(x)
        out = self.relu1(out)
        shortcut = self.shortcut(out) if self.shortcut is not None else x
        out = self.conv1(out)
        out = self.bn2(out)
        out = self.relu2(out)
        out = self.conv2(out)
        return out + shortcut


class ResNetStem(nn.Module):
    def __init__(self, in_channels, base_channels=64, num_blocks=5):
        super().__init__()
        self.conv_in = nn.Conv2d(in_channels, base_channels, kernel_size=3,
                                stride=1, padding=1, bias=False)
        self.blocks = nn.Sequential(
            *[PreActBlock(base_channels, base_channels, stride=1)
              for _ in range(num_blocks)]
        )
        self.bn_out = nn.BatchNorm2d(base_channels)
        self.relu_out = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv_in(x)
        x = self.blocks(x)
        x = self.bn_out(x)
        x = self.relu_out(x)
        return x


class FusionResNetDirichlet(nn.Module):
    def __init__(self, day_channels=7, night_channels=1, n_comp=4,
                 base_channels=64, day_blocks=3, night_blocks=3, shared_blocks=5):
        super().__init__()
        self.day_stem = ResNetStem(day_channels, base_channels, day_blocks)
        self.night_stem = ResNetStem(night_channels, base_channels, night_blocks)

        self.shared_in_conv = nn.Conv2d(2 * base_channels, base_channels,
                                       kernel_size=3, stride=1, padding=1, bias=False)
        self.shared_blocks = nn.Sequential(
            *[PreActBlock(base_channels, base_channels, stride=1)
              for _ in range(shared_blocks)]
        )
        self.shared_bn = nn.BatchNorm2d(base_channels)
        self.shared_relu = nn.ReLU(inplace=True)

        self.head_conv = nn.Conv2d(base_channels, base_channels,
                                   kernel_size=3, stride=1, padding=1, bias=False)
        self.head_bn = nn.BatchNorm2d(base_channels)
        self.head_relu = nn.ReLU(inplace=True)
        self.head_drop = nn.Dropout2d(0.1)
        self.head_out = nn.Conv2d(base_channels, n_comp, kernel_size=1, bias=True)

    def forward(self, day, night):
        d = self.day_stem(day)
        n = self.night_stem(night)
        
        x = torch.cat([d, n], dim=1)
        x = self.shared_in_conv(x)
        x = self.shared_blocks(x)
        x = self.shared_bn(x)
        x = self.shared_relu(x)
        
        x = self.head_conv(x)
        x = self.head_bn(x)
        x = self.head_relu(x)
        x = self.head_drop(x)
        alpha_raw = self.head_out(x)
        
        alpha = F.softplus(alpha_raw) + 1.0
        
        return alpha

print("âœ… æ¨¡å‹æ¶æ„å®šä¹‰å®Œæˆ")


# %% ============================================================================
#                           ç¬¬å››éƒ¨åˆ†ï¼šé¢„ç­›é€‰å‡½æ•°ï¼ˆå¿«é€Ÿç‰ˆï¼‰
# ==============================================================================

def prefilter_valid_windows_fast(reference_tif, windows, valid_threshold=0,   #0.001, 
                                 cache_path=None):
    """
    å¿«é€Ÿé¢„ç­›é€‰æœ‰æ•ˆçª—å£ï¼ˆé™é‡‡æ ·æ–¹æ³•ï¼‰
    
    å‚æ•°:
        reference_tif: å‚è€ƒæ …æ ¼æ–‡ä»¶
        windows: æ‰€æœ‰çª—å£åˆ—è¡¨
        valid_threshold: æœ‰æ•ˆåƒå…ƒæ¯”ä¾‹é˜ˆå€¼
        cache_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        valid_windows: æœ‰æ•ˆçª—å£åˆ—è¡¨
    """
    # æ£€æŸ¥ç¼“å­˜
    if cache_path and os.path.exists(cache_path):
        print(f"\nğŸ” åŠ è½½é¢„ç­›é€‰ç¼“å­˜...")
        try:
            data = np.load(cache_path)
            valid_windows = [tuple(w) for w in data['windows']]
            print(f"   âœ… ä»ç¼“å­˜åŠ è½½ {len(valid_windows):,} ä¸ªæœ‰æ•ˆçª—å£")
            print(f"   è¿‡æ»¤æ¯”ä¾‹: {(1 - len(valid_windows)/len(windows))*100:.1f}%")
            return valid_windows
        except Exception as e:
            print(f"   âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
    
    print("\nğŸ” å¿«é€Ÿé¢„ç­›é€‰æœ‰æ•ˆçª—å£ï¼ˆé™é‡‡æ ·æ–¹æ³•ï¼‰...")
    print(f"   æœ‰æ•ˆé˜ˆå€¼: {valid_threshold*100:.0f}%")
    
    # æ‰“å¼€å‚è€ƒå½±åƒ
    with rasterio.open(reference_tif) as src:
        height = src.height
        width = src.width
        
        print(f"   å½±åƒå°ºå¯¸: {height:,} Ã— {width:,}")
        
        # æ­¥éª¤1: ç”Ÿæˆé™é‡‡æ ·æ©è†œ
        print("\n   [æ­¥éª¤1/2] ç”Ÿæˆé™é‡‡æ ·æ©è†œï¼ˆåŠ é€Ÿåˆ¤æ–­ï¼‰...")
        
        # é™é‡‡æ ·å› å­ï¼ˆ4å€é™é‡‡æ ·ï¼Œé€Ÿåº¦æå‡16å€ï¼‰
        downsample_factor = 4
        small_height = (height + downsample_factor - 1) // downsample_factor
        small_width = (width + downsample_factor - 1) // downsample_factor
        
        # è¯»å–é™é‡‡æ ·å½±åƒï¼ˆåªéœ€å‡ ç§’ï¼‰
        small_data = src.read(
            1,
            out_shape=(small_height, small_width),
            resampling=rasterio.enums.Resampling.nearest,
            masked=True
        )
        
        # ç”Ÿæˆæœ‰æ•ˆæ€§æ©è†œ
        valid_mask_small = ~small_data.mask
        
        print(f"      é™é‡‡æ ·å½±åƒ: {small_height:,} Ã— {small_width:,}")
        print(f"      æœ‰æ•ˆæ¯”ä¾‹: {valid_mask_small.sum() / valid_mask_small.size * 100:.1f}%")
        print(f"      âœ… æ©è†œç”Ÿæˆå®Œæˆ")
    
    # æ­¥éª¤2: åŸºäºé™é‡‡æ ·æ©è†œå¿«é€Ÿç­›é€‰çª—å£
    print("\n   [æ­¥éª¤2/2] å¿«é€Ÿç­›é€‰æœ‰æ•ˆçª—å£...")
    
    valid_windows = []
    
    for row, col, win_h, win_w in tqdm(windows, desc="      ç­›é€‰è¿›åº¦"):
        # è®¡ç®—çª—å£åœ¨é™é‡‡æ ·å›¾ä¸­çš„ä½ç½®
        small_row_start = row // downsample_factor
        small_col_start = col // downsample_factor
        small_row_end = min((row + win_h + downsample_factor - 1) // downsample_factor, 
                           small_height)
        small_col_end = min((col + win_w + downsample_factor - 1) // downsample_factor, 
                           small_width)
        
        # æ£€æŸ¥é™é‡‡æ ·åŒºåŸŸçš„æœ‰æ•ˆæ¯”ä¾‹ï¼ˆæ•°ç»„æŸ¥è¯¢ï¼Œæå¿«ï¼‰
        if small_row_end > small_row_start and small_col_end > small_col_start:
            small_region = valid_mask_small[
                small_row_start:small_row_end,
                small_col_start:small_col_end
            ]
            
            if small_region.size > 0:
                valid_ratio = small_region.sum() / small_region.size
                
                # ä½¿ç”¨ç¨å¾®å®½æ¾çš„é˜ˆå€¼ï¼ˆå› ä¸ºæ˜¯é™é‡‡æ ·ä¼°è®¡ï¼‰
                if valid_ratio >= valid_threshold * 0.8:
                    valid_windows.append((row, col, win_h, win_w))
    
    print(f"\n   âœ… ç­›é€‰å®Œæˆ:")
    print(f"      åŸå§‹çª—å£: {len(windows):,}")
    print(f"      æœ‰æ•ˆçª—å£: {len(valid_windows):,}")
    print(f"      è¿‡æ»¤æ¯”ä¾‹: {(1 - len(valid_windows)/len(windows))*100:.1f}%")
    print(f"      é¢„è®¡åŠ é€Ÿ: {len(windows)/max(len(valid_windows), 1):.1f}x")
    
    # ä¿å­˜ç¼“å­˜
    if cache_path:
        try:
            print(f"\n   ğŸ’¾ ä¿å­˜ç¼“å­˜...")
            np.savez_compressed(
                cache_path,
                windows=np.array(valid_windows, dtype=np.int32)
            )
            cache_size = os.path.getsize(cache_path) / 1e6
            print(f"   âœ… ç¼“å­˜å·²ä¿å­˜: {os.path.basename(cache_path)} ({cache_size:.1f} MB)")
        except Exception as e:
            print(f"   âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    return valid_windows

# %% ============================================================================
#                           ç¬¬äº”éƒ¨åˆ†ï¼šæ•°æ®é›†
# ==============================================================================

class InferenceDataset(Dataset):
    """æ¨ç†æ•°æ®é›†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    def __init__(self, windows, day_tifs, night_tif, patch_size=64):
        self.windows = windows
        self.day_tifs = day_tifs
        self.night_tif = night_tif
        self.patch_size = patch_size
        
        # å»¶è¿Ÿåˆå§‹åŒ–
        self.day_srcs = None
        self.night_src = None
    
    def _init_sources(self):
        """åœ¨workerè¿›ç¨‹ä¸­åˆå§‹åŒ–æ–‡ä»¶å¥æŸ„"""
        if self.day_srcs is None:
            self.day_srcs = [rasterio.open(p) for p in self.day_tifs]
            self.night_src = rasterio.open(self.night_tif)
    
    def __len__(self):
        return len(self.windows)
    
    def __getitem__(self, idx):
        self._init_sources()
        
        row, col, win_h, win_w = self.windows[idx]
        window = Window(col, row, win_w, win_h)
        ps = self.patch_size
        
        # è¯»å–ç¬¬ä¸€ä¸ªæ³¢æ®µæ£€æŸ¥æœ‰æ•ˆæ€§
        arr0 = self.day_srcs[0].read(1, window=window, boundless=True, masked=True)
        
        if arr0.mask.all():
            # è¿”å›æ— æ•ˆæ ‡è®°
            return {
                'day': torch.zeros((len(self.day_tifs), ps, ps), dtype=torch.float32),
                'night': torch.zeros((1, ps, ps), dtype=torch.float32),
                'valid_mask': torch.zeros((ps, ps), dtype=torch.float32),
                'meta': (row, col, 0, 0),
                'is_valid': False
            }
        
        valid_mask = (~arr0.mask).astype(np.float32)
        day_stack = [arr0.filled(0).astype(np.float32)]
        
        # è¯»å–å…¶ä»–æ³¢æ®µ
        for src in self.day_srcs[1:]:
            arr = src.read(1, window=window, boundless=True, masked=True)
            day_stack.append(arr.filled(0).astype(np.float32))
        
        night_arr = self.night_src.read(1, window=window, boundless=True, masked=True)
        night_arr = night_arr.filled(0).astype(np.float32)
        
        # Padding
        if win_h != ps or win_w != ps:
            pad_day = np.zeros((len(day_stack), ps, ps), dtype=np.float32)
            pad_night = np.zeros((1, ps, ps), dtype=np.float32)
            pad_valid = np.zeros((ps, ps), dtype=np.float32)
            
            pad_day[:, :win_h, :win_w] = np.stack(day_stack, axis=0)
            pad_night[:, :win_h, :win_w] = night_arr[np.newaxis, :, :]
            pad_valid[:win_h, :win_w] = valid_mask
            
            day_arr = pad_day
            night_arr = pad_night
            valid_mask = pad_valid
        else:
            day_arr = np.stack(day_stack, axis=0)
            night_arr = night_arr[np.newaxis, :, :]
        
        return {
            'day': torch.from_numpy(day_arr),
            'night': torch.from_numpy(night_arr),
            'valid_mask': torch.from_numpy(valid_mask),
            'meta': (row, col, win_h, win_w),
            'is_valid': True
        }


# %% ============================================================================
#                           ç¬¬å…­éƒ¨åˆ†ï¼šå·¥å…·å‡½æ•°
# ==============================================================================

def create_weight_patch(patch_size):
    """åˆ›å»ºæƒé‡çŸ©é˜µ"""
    weight = np.ones((patch_size, patch_size), dtype=np.float32)
    
    for i in range(patch_size):
        for j in range(patch_size):
            dist_i = min(i, patch_size - 1 - i) / (patch_size / 2)
            dist_j = min(j, patch_size - 1 - j) / (patch_size / 2)
            weight[i, j] = min(dist_i, dist_j)
    
    return weight

class CheckpointManager:
    """æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    def __init__(self, checkpoint_dir):
        self.checkpoint_dir = checkpoint_dir
        self.checkpoint_path = os.path.join(checkpoint_dir, "checkpoint.npz")
        self.meta_path = os.path.join(checkpoint_dir, "checkpoint_meta.json")
    
    def save(self, sum_comp, sum_weight, progress, total):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            np.savez_compressed(
                self.checkpoint_path,
                sum_comp=sum_comp,
                sum_weight=sum_weight
            )
            
            meta = {
                'progress': int(progress),
                'total': int(total),
                'progress_percent': progress / total * 100,
                'timestamp': time.time()
            }
            
            with open(self.meta_path, 'w') as f:
                json.dump(meta, f, indent=2)
            
            return True
        except Exception as e:
            print(f"\n   âš ï¸ æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def load(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        print(f"\nğŸ” æ£€æŸ¥æ£€æŸ¥ç‚¹...")
        print(f"   è·¯å¾„: {self.checkpoint_path}")
        
        if not os.path.exists(self.checkpoint_path):
            print("   âŒ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨ï¼Œä»å¤´å¼€å§‹")
            return None, None, 0
        
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        file_size_gb = os.path.getsize(self.checkpoint_path) / 1e9
        print(f"   âœ… æ£€æŸ¥ç‚¹å­˜åœ¨ ({file_size_gb:.2f} GB)")
        
        try:
            # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¯èƒ½éœ€è¦1-3åˆ†é’Ÿï¼‰
            print("   â³ æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆè§£å‹ä¸­ï¼Œè¯·ç¨å€™ï¼‰...")
            load_start = time.time()
            
            data = np.load(self.checkpoint_path)
            sum_comp = data['sum_comp']
            sum_weight = data['sum_weight']
            
            load_time = time.time() - load_start
            print(f"   âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ (è€—æ—¶ {load_time:.1f} ç§’)")
            print(f"      sum_comp: {sum_comp.shape}, {sum_comp.nbytes/1e9:.2f}GB")
            print(f"      sum_weight: {sum_weight.shape}, {sum_weight.nbytes/1e9:.2f}GB")
            
            # åŠ è½½å…ƒä¿¡æ¯
            if os.path.exists(self.meta_path):
                with open(self.meta_path, 'r') as f:
                    meta = json.load(f)
                progress = meta['progress']
                
                print(f"\n   ğŸ“Š ç»§ç»­æ¨ç†:")
                print(f"      å·²å®Œæˆ: {progress:,} / {meta['total']:,} "
                      f"({meta['progress_percent']:.1f}%)")
                
                elapsed = time.time() - meta['timestamp']
                print(f"      ä¸Šæ¬¡ä¿å­˜: {elapsed/3600:.1f} å°æ—¶å‰")
                
                remaining = meta['total'] - progress
                print(f"      å‰©ä½™çª—å£: {remaining:,}")
            else:
                print("   âš ï¸ å…ƒä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»å¤´å¼€å§‹")
                progress = 0
            
            return sum_comp, sum_weight, progress
        
        except Exception as e:
            print(f"\n   âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None, 0
    
    def clean(self):
        """æ¸…ç†æ£€æŸ¥ç‚¹"""
        try:
            if os.path.exists(self.checkpoint_path):
                os.remove(self.checkpoint_path)
            if os.path.exists(self.meta_path):
                os.remove(self.meta_path)
            return True
        except:
            return False


# %% ============================================================================
#                           ç¬¬ä¸ƒéƒ¨åˆ†ï¼šä¸»æ¨ç†å‡½æ•°
# ==============================================================================

def infer_full_raster_optimized(model_path, tag=None):
    """
    å…¨å›¾æ¨ç†å‡½æ•°ï¼ˆå•GPUä¼˜åŒ–ç‰ˆï¼‰
    """
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹å…¨å›¾æ¨ç†ï¼ˆå•GPUä¼˜åŒ–ç‰ˆï¼‰")
    print("=" * 80)
    
    start_time = time.time()
    
    # 1. è·å–å½±åƒä¿¡æ¯
    print("\nğŸ“‚ è¯»å–å½±åƒä¿¡æ¯...")
    with rasterio.open(DAY_TIFS[0]) as src:
        height = src.height
        width = src.width
        transform = src.transform
        crs = src.crs
        meta = src.meta.copy()
    
    print(f"   å½±åƒå°ºå¯¸: {height:,} Ã— {width:,} åƒç´ ")
    
    # 2. ç”Ÿæˆæ‰€æœ‰çª—å£
    print("\nğŸ”„ ç”Ÿæˆæ¨ç†çª—å£...")
    all_windows = []
    for row in range(0, height, STEP):
        for col in range(0, width, STEP):
            win_h = min(PATCH_SIZE, height - row)
            win_w = min(PATCH_SIZE, width - col)
            all_windows.append((row, col, win_h, win_w))
    
    print(f"   åŸå§‹çª—å£: {len(all_windows):,}")
    
    # 3. é¢„ç­›é€‰ï¼ˆå¸¦ç¼“å­˜ï¼‰
    cache_path = os.path.join(OUT_DIR, "valid_windows_cache.npz")


    # åœ¨ infer_full_raster_optimized å‡½æ•°ä¸­
    windows = prefilter_valid_windows_fast(  # â† ç¡®ä¿è°ƒç”¨çš„æ˜¯ _fast ç‰ˆæœ¬
        DAY_TIFS[0],
        all_windows,
        valid_threshold=VALID_THRESHOLD,
        cache_path=cache_path
)
    
    total_windows = len(windows)
    
    # ä¼°ç®—æ—¶é—´
    estimated_batches = (total_windows + BATCH_SIZE - 1) // BATCH_SIZE
    estimated_minutes = estimated_batches / 30  # å‡è®¾30 batch/åˆ†é’Ÿ
    print(f"\n   é¢„è®¡æ¨ç†æ—¶é—´: {estimated_minutes:.1f} åˆ†é’Ÿ ({estimated_minutes/60:.2f} å°æ—¶)")
    
    # 4. åŠ è½½æ¨¡å‹
    print("\nğŸ”§ åŠ è½½æ¨¡å‹...")
    model = FusionResNetDirichlet(
        day_channels=len(DAY_TIFS),
        night_channels=1,
        n_comp=len(TARGET_FIELDS),
        base_channels=64,
        day_blocks=3,
        night_blocks=3,
        shared_blocks=5,
    )
    
    try:
        state = torch.load(model_path, map_location='cpu')
        model.load_state_dict(state)
        model = model.cuda()
        model.eval()
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    print(f"   âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU")
    total_params = sum(p.numel() for p in model.parameters())
    print(f"   å‚æ•°é‡: {total_params/1e6:.2f}M")
    
    # 5. åˆå§‹åŒ–ç´¯ç§¯æ•°ç»„
    print("\nğŸ’¾ åˆ†é…å†…å­˜...")
    n_comp = len(TARGET_FIELDS)
    
    try:
        sum_comp = np.zeros((n_comp, height, width), dtype=np.float32)
        sum_weight = np.zeros((height, width), dtype=np.float32)
    except MemoryError:
        print("âŒ å†…å­˜ä¸è¶³ï¼")
        sys.exit(1)
    
    memory_gb = (sum_comp.nbytes + sum_weight.nbytes) / 1e9
    print(f"   å·²åˆ†é…: {memory_gb:.2f} GB")
    
    # 6. å‡†å¤‡æƒé‡çŸ©é˜µ
    weight_patch = create_weight_patch(PATCH_SIZE)
    
# 7. æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    checkpoint_mgr = CheckpointManager(OUT_DIR)
    
    # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
    loaded_comp, loaded_weight, start_idx = checkpoint_mgr.load()
    
    if loaded_comp is not None:
        print("\n   ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤...")
        sum_comp[:] = loaded_comp
        sum_weight[:] = loaded_weight
        print(f"   âœ… ç´¯ç§¯æ•°ç»„å·²æ¢å¤")
        print(f"   â­ï¸  å°†ä»ç¬¬ {start_idx:,} ä¸ªçª—å£ç»§ç»­")
    else:
        start_idx = 0
        print("   â„¹ï¸  ä»å¤´å¼€å§‹æ¨ç†")
    
    # 8. åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
    print("\nâš™ï¸ å‡†å¤‡æ•°æ®åŠ è½½å™¨...")
    dataset = InferenceDataset(windows, DAY_TIFS, NIGHT_TIF, PATCH_SIZE)
    
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=NUM_WORKERS,
        prefetch_factor=PREFETCH_FACTOR if NUM_WORKERS > 0 else None,
        pin_memory=PIN_MEMORY,
        persistent_workers=True if NUM_WORKERS > 0 else False
    )
    
    print(f"   DataLoaderé…ç½®:")
    print(f"      Batch size: {BATCH_SIZE}")
    print(f"      Workers: {NUM_WORKERS}")
    print(f"      Prefetch: {PREFETCH_FACTOR}")
    
    # 9. æ¨ç†å¾ªç¯
    print("\nâ³ å¼€å§‹æ¨ç†...")
    processed_windows = start_idx  # ä»æ£€æŸ¥ç‚¹è¿›åº¦å¼€å§‹è®¡æ•°
    
    # è®¡ç®—èµ·å§‹batch
    start_batch = start_idx // BATCH_SIZE
    
    print(f"   ä»ç¬¬ {start_batch} ä¸ªbatchå¼€å§‹ï¼ˆè·³è¿‡å‰ {start_batch} ä¸ªï¼‰")
    
    with torch.no_grad():
        pbar = tqdm(
            enumerate(dataloader),
            total=len(dataloader),
            desc="æ¨ç†è¿›åº¦",
            initial=start_batch
        )
        
        for batch_idx, batch in pbar:
            # è·³è¿‡å·²å¤„ç†çš„batch
            if batch_idx < start_batch:
                continue
            
            # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬
            valid_indices = batch['is_valid']
            if not valid_indices.any():
                continue
            
            day_data = batch['day'][valid_indices].cuda(non_blocking=True)
            night_data = batch['night'][valid_indices].cuda(non_blocking=True)
            
            # æ··åˆç²¾åº¦æ¨ç†
            if USE_AMP:
                with autocast():
                    alpha = model(day_data, night_data)
            else:
                alpha = model(day_data, night_data)
            
            alpha = alpha.cpu().numpy()
            
            # å†™å›ç»“æœ
            valid_idx = 0
            for i, is_valid in enumerate(valid_indices):
                if not is_valid:
                    continue
                
                row = int(batch['meta'][0][i])
                col = int(batch['meta'][1][i])
                win_h = int(batch['meta'][2][i])
                win_w = int(batch['meta'][3][i])
                
                if win_h == 0:
                    continue
                
                valid_mask = batch['valid_mask'][i].numpy()
                comp = alpha[valid_idx]
                comp = comp / np.clip(comp.sum(axis=0, keepdims=True), 1e-6, None)
                
                w_full = weight_patch * valid_mask
                w = w_full[:win_h, :win_w]
                
                sum_comp[:, row:row+win_h, col:col+win_w] += comp[:, :win_h, :win_w] * w
                sum_weight[row:row+win_h, col:col+win_w] += w
                
                valid_idx += 1
                processed_windows += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'windows': f'{processed_windows:,}',
                'GPU_mem': f'{torch.cuda.memory_allocated()/1e9:.1f}GB'
            })
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if AUTO_SAVE and batch_idx % CHECKPOINT_INTERVAL == 0 and batch_idx > start_batch:
                checkpoint_mgr.save(
                    sum_comp, sum_weight,
                    processed_windows,
                    total_windows
                )
    
    elapsed_time = time.time() - start_time
    print(f"\nâœ… æ¨ç†å®Œæˆ! è€—æ—¶: {elapsed_time/3600:.2f} å°æ—¶")
    print(f"   å¤„ç†çª—å£: {processed_windows:,}")
    
    # ========== æ¨ç†å®Œæˆåç«‹å³æ¸…ç†å†…å­˜ ==========
    print("\nğŸ§¹ æ¸…ç†æ¨ç†èµ„æº...")
    
    # 1. å…³é—­DataLoaderçš„workerè¿›ç¨‹
    try:
        del dataloader
        print("   âœ… DataLoaderå·²æ¸…ç†")
    except:
        pass
    
    # 2. åˆ é™¤Dataset
    try:
        del dataset
        print("   âœ… Datasetå·²æ¸…ç†")
    except:
        pass
    
    # 3. åˆ é™¤æ¨¡å‹
    try:
        del model
        print("   âœ… æ¨¡å‹å·²æ¸…ç†")
    except:
        pass
    
    # 4. æ¸…ç©ºGPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("   âœ… GPUç¼“å­˜å·²æ¸…ç©º")
    
    # 5. å¼ºåˆ¶åƒåœ¾å›æ”¶
    import gc
    gc.collect()
    print("   âœ… åƒåœ¾å›æ”¶å®Œæˆ")
    
    # 6. æ˜¾ç¤ºå½“å‰å†…å­˜çŠ¶æ€
    import psutil
    mem = psutil.virtual_memory()
    print(f"   å¯ç”¨å†…å­˜: {mem.available / 1e9:.1f} GB / {mem.total / 1e9:.1f} GB")
    
    # 10. å½’ä¸€åŒ–å¹¶å†™å‡ºï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆï¼šå®Œå…¨é¿å…å‰¯æœ¬ï¼‰
    print(f"\nğŸ“Š å½’ä¸€åŒ–å¹¶å†™å‡ºç»“æœï¼ˆé€ç»„åˆ†å¤„ç†+åˆ†å—å†™å‡ºï¼‰...")
    
    meta.update(count=1, dtype='float32', compress='lzw', nodata=-9999)
    output_files = []
    
    CHUNK_ROWS = 5000  # æ¯æ¬¡å¤„ç†5000è¡Œ
    
    for k, name in enumerate(TARGET_FIELDS):
        print(f"\n   [{k+1}/{len(TARGET_FIELDS)}] {name}")
        
        # åˆ†å—å½’ä¸€åŒ–å¹¶å†™å‡ºï¼ˆå®Œå…¨é¿å…åˆ›å»ºå¤§æ•°ç»„ï¼‰
        fname = f"pred_{tag}_{name}_2020_90m.tif" if tag else f"pred_{name}_2020_90m.tif"
        out_path = os.path.join(OUT_DIR, fname)
        
        print(f"      åˆ†å—å½’ä¸€åŒ–å¹¶å†™å‡º: {fname}...")
        
        valid_count = 0
        sum_values = 0.0
        sum_squares = 0.0
        
        with rasterio.open(out_path, 'w', **meta) as dst:
            num_chunks = (height + CHUNK_ROWS - 1) // CHUNK_ROWS
            
            for i in tqdm(range(num_chunks), desc=f"      å¤„ç†{name}", leave=False):
                start_row = i * CHUNK_ROWS
                end_row = min(start_row + CHUNK_ROWS, height)
                
                # è¯»å–å—ï¼ˆå¼•ç”¨ï¼Œä¸å¤åˆ¶ï¼‰
                comp_chunk = sum_comp[k, start_row:end_row, :]
                weight_chunk = sum_weight[start_row:end_row, :]
                
                # åˆ›å»ºå®‰å…¨é™¤æ•°ï¼ˆåªåœ¨å°å—ä¸Šæ“ä½œï¼‰
                weight_safe = np.where(weight_chunk > 0, weight_chunk, 1.0)
                
                # å½’ä¸€åŒ–ï¼ˆåªåœ¨å°å—ä¸Šæ“ä½œï¼‰
                normalized_chunk = comp_chunk / weight_safe
                
                # è®¾ç½®NoData
                normalized_chunk[weight_chunk == 0] = -9999
                
                # ç»Ÿè®¡ï¼ˆç´¯ç§¯ï¼‰
                valid_mask = (weight_chunk > 0)
                if valid_mask.any():
                    valid_values = normalized_chunk[valid_mask]
                    valid_count += len(valid_values)
                    sum_values += valid_values.sum()
                    sum_squares += (valid_values ** 2).sum()
                
                # å†™å‡ºè¿™ä¸€å—
                dst.write(normalized_chunk, 1, window=Window(0, start_row, width, end_row - start_row))
                
                # é‡Šæ”¾
                del weight_safe, normalized_chunk
        
        # è®¡ç®—ç»Ÿè®¡é‡
        if valid_count > 0:
            mean = sum_values / valid_count
            variance = (sum_squares / valid_count) - (mean ** 2)
            std = np.sqrt(max(variance, 0))
            print(f"      ç»Ÿè®¡: Î¼={mean:.4f}, Ïƒ={std:.4f}, N={valid_count:,}")
        
        file_size = os.path.getsize(out_path) / 1e6
        print(f"      âœ… å®Œæˆ ({file_size:.1f} MB)")
        
        output_files.append(out_path)
    
    # æ¸…ç†å†…å­˜
    print("\n   æ¸…ç†å†…å­˜...")
    del sum_comp, sum_weight
    import gc
    gc.collect()
    
    # ç°åœ¨æ‰æ¸…ç†æ£€æŸ¥ç‚¹ï¼ˆä¸€åˆ‡æˆåŠŸåï¼‰
    # ========== ä¿ç•™æ£€æŸ¥ç‚¹ä½œä¸ºå¤‡ä»½ï¼ˆä¸æ¸…ç†ï¼‰==========
    # checkpoint_mgr.clean()  # æ³¨é‡Šæ‰ï¼Œä¿ç•™æ£€æŸ¥ç‚¹
    print("   âœ… æ£€æŸ¥ç‚¹å·²ä¿ç•™å¤‡ä»½")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ å…¨å›¾æ¨ç†å®Œæˆï¼ï¼ˆå•GPUä¼˜åŒ–ç‰ˆï¼‰")
    print("=" * 80)
    
    return output_files

# %% ============================================================================
#                           ç¬¬å…«éƒ¨åˆ†ï¼šä¸»ç¨‹åº
# ==============================================================================

def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "=" * 80)
    print("å¼€å§‹æ‰§è¡Œå…¨å›¾æ¨ç†")
    print("=" * 80)
    
    # æ‰§è¡Œæ¨ç†
    output_files = infer_full_raster_optimized(
        model_path=BEST_MODEL_PATH,
        tag="optimized"
    )
    
    # è¾“å‡ºæ€»ç»“
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    for f in output_files:
        print(f"   {f}")
    
    print("\nğŸ’¡ ä¼˜åŒ–æ•ˆæœ:")
    print("   âœ… é¢„ç­›é€‰: è¿‡æ»¤82.5%æ— æ•ˆçª—å£")
    print("   âœ… æ··åˆç²¾åº¦: FP16åŠ é€Ÿ")
    print("   âœ… DataLoader: å¤šè¿›ç¨‹å¹¶è¡Œè¯»å–")
    print("   âœ… æ–­ç‚¹ç»­ä¼ : æ”¯æŒéšæ—¶ä¸­æ–­æ¢å¤")
    print("   âœ… å¤§Batch: å……åˆ†åˆ©ç”¨24GBæ˜¾å­˜")
    print("\n   é¢„æœŸ: 8å°æ—¶ â†’ 1-1.5å°æ—¶")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ¨ç†ï¼ˆæ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†è‡ªåŠ¨æ¢å¤ï¼‰")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)