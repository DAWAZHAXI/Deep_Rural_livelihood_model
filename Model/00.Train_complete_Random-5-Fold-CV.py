#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# %% [markdown]ç¬¬ä¸€éƒ¨åˆ†ï¼šå®Œæ•´è®­ç»ƒè„šæœ¬
# # å®Œæ•´è®­ç»ƒè„šæœ¬ï¼ˆå†…å­˜ç¼“å­˜ä¼˜åŒ–ç‰ˆï¼‰
# ## ä¼˜åŒ–ï¼šæ•°æ®é¢„åŠ è½½åˆ°å†…å­˜ + å¤šçº¿ç¨‹ + è¿›åº¦æ¡

# %% å¯¼å…¥æ‰€æœ‰åº“
import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, Subset, DataLoader
from sklearn.model_selection import KFold
from sklearn.metrics import r2_score
import rasterio
import geopandas as gpd
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

print("âœ… åº“å¯¼å…¥æˆåŠŸ")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

# %% [markdown]
# ## ç¬¬ä¸€éƒ¨åˆ†ï¼šå†…å­˜ç¼“å­˜ç‰ˆæ•°æ®é›†

# %% å®šä¹‰ PatchPointDatasetï¼ˆå†…å­˜ç¼“å­˜ç‰ˆï¼‰
# %% [markdown]
# ## ç¬¬ä¸€éƒ¨åˆ†ï¼šå†…å­˜ç¼“å­˜ç‰ˆæ•°æ®é›†

# %% å®šä¹‰ PatchPointDatasetï¼ˆå†…å­˜ç¼“å­˜ç‰ˆï¼‰
class PatchPointDataset(Dataset):
    """ç‚¹æ ·æœ¬æ•°æ®é›†ï¼ˆå†…å­˜ç¼“å­˜ç‰ˆ - è§£å†³picklingé—®é¢˜ï¼‰"""

    def __init__(self, shp_path, day_paths, night_path, target_fields,
                 patch_size=64, mode='train', check_valid=True, 
                 max_samples=None, cache_data=True):
        super().__init__()

        print(f"ğŸ“‚ è¯»å–shapefile: {os.path.basename(shp_path)}")
        self.gdf = gpd.read_file(shp_path).reset_index(drop=True)

        if max_samples is not None and max_samples < len(self.gdf):
            print(f"âš ï¸ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªä½¿ç”¨å‰ {max_samples} ä¸ªæ ·æœ¬")
            self.gdf = self.gdf.iloc[:max_samples]

        self.day_paths = day_paths
        self.night_path = night_path
        self.target_fields = target_fields
        self.patch = patch_size
        self.mode = mode
        self.cache_data = cache_data

        print(f"ğŸ”§ æ‰“å¼€æ …æ ¼æ–‡ä»¶...")
        self.day_srcs = [rasterio.open(p) for p in self.day_paths]
        self.night_src = rasterio.open(self.night_path)

        self.height = self.day_srcs[0].height
        self.width = self.day_srcs[0].width
        self.transform = self.day_srcs[0].transform

        print(f"   å½±åƒå°ºå¯¸: {self.height} Ã— {self.width}")

        if check_valid:
            self.valid_idx, self.cached_patches = self._build_valid_index_and_cache()
        else:
            self.valid_idx = list(range(len(self.gdf)))
            self.cached_patches = None
            print(f"âœ… è·³è¿‡é¢„ç­›ï¼Œä½¿ç”¨æ‰€æœ‰ {len(self.gdf)} ä¸ªæ ·æœ¬")

        if len(self.valid_idx) == 0:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨æ ·æœ¬")

        # å¦‚æœæ•°æ®å·²ç¼“å­˜ï¼Œå…³é—­æ–‡ä»¶å¥æŸ„
        if self.cache_data and self.cached_patches is not None:
            print("ğŸ’¾ æ•°æ®å·²ç¼“å­˜åˆ°å†…å­˜ï¼Œå…³é—­æ …æ ¼æ–‡ä»¶")
            for s in self.day_srcs:
                s.close()
            self.night_src.close()
            self.day_srcs = None
            self.night_src = None

    def _check_and_load_sample(self, idx):
        """æ£€æŸ¥æ ·æœ¬æœ‰æ•ˆæ€§å¹¶åŠ è½½æ•°æ®åˆ°å†…å­˜"""
        try:
            row = self.gdf.iloc[idx]
            geom = row.geometry
            if geom is None or geom.is_empty:
                return None
            if geom.geom_type != "Point":
                geom = geom.centroid
            x, y = geom.x, geom.y

            half = self.patch // 2
            first_src = self.day_srcs[0]
            r, c = rasterio.transform.rowcol(first_src.transform, x, y)

            if (r < half or c < half or 
                r >= first_src.height - half or 
                c >= first_src.width - half):
                return None

            window = rasterio.windows.Window(c - half, r - half, self.patch, self.patch)

            # è¯»å–æ—¥é—´æ•°æ®
            day_stack = []
            for src in self.day_srcs:
                arr = src.read(1, window=window, boundless=True, masked=True)
                if arr.mask.all() or np.isnan(arr.filled(0)).all():
                    return None
                day_stack.append(arr.filled(0).astype(np.float32))

            day_arr = np.stack(day_stack, axis=0)

            # è¯»å–å¤œå…‰æ•°æ®
            night_arr = self.night_src.read(1, window=window, boundless=True, masked=True)
            if night_arr.mask.all() or np.isnan(night_arr.filled(0)).all():
                return None
            night_arr = night_arr.filled(0).astype(np.float32)[np.newaxis, :, :]

            # æ ‡ç­¾æ£€æŸ¥
            vals = [row[f] for f in self.target_fields]
            if any((v is None) or (isinstance(v, float) and np.isnan(v)) for v in vals):
                return None

            # å¤„ç†æ ‡ç­¾
            y = np.array(vals, dtype=np.float32)
            s = y.sum()
            if s > 1e-6:
                y = y / s
            else:
                y = np.array([1.0] + [0.0] * (len(self.target_fields) - 1), dtype=np.float32)

            return {
                'day': day_arr,
                'night': night_arr,
                'y': y
            }

        except Exception:
            return None

    def _build_valid_index_and_cache(self):
        """å¤šçº¿ç¨‹é¢„ç­›å¹¶ç¼“å­˜æ•°æ®åˆ°å†…å­˜"""
        print("â³ å¤šçº¿ç¨‹é¢„ç­›å¹¶ç¼“å­˜æ•°æ®åˆ°å†…å­˜...")

        valid_idx = []
        cached_data = {} if self.cache_data else None

        max_workers = min(8, os.cpu_count() or 4)

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self._check_and_load_sample, idx): idx 
                      for idx in range(len(self.gdf))}

            for future in tqdm(as_completed(futures), 
                             total=len(futures), 
                             desc="åŠ è½½æ•°æ®"):
                idx = futures[future]
                result = future.result()

                if result is not None:
                    valid_idx.append(idx)
                    if self.cache_data:
                        cached_data[idx] = result

        valid_idx.sort()

        # è®¡ç®—å†…å­˜å ç”¨
        if self.cache_data and cached_data:
            sample_size = (cached_data[valid_idx[0]]['day'].nbytes + 
                          cached_data[valid_idx[0]]['night'].nbytes +
                          cached_data[valid_idx[0]]['y'].nbytes) / 1e6
            total_size = sample_size * len(valid_idx)
            print(f"âœ… æœ‰æ•ˆæ ·æœ¬: {len(valid_idx)}/{len(self.gdf)} ({len(valid_idx)/len(self.gdf)*100:.1f}%)")
            print(f"   å†…å­˜å ç”¨: {total_size:.1f} MB ({sample_size:.2f} MB/æ ·æœ¬)")
        else:
            print(f"âœ… æœ‰æ•ˆæ ·æœ¬: {len(valid_idx)}/{len(self.gdf)} ({len(valid_idx)/len(self.gdf)*100:.1f}%)")

        return valid_idx, cached_data

    def __len__(self):
        return len(self.valid_idx)

    def set_mode(self, mode: str):
        """åˆ‡æ¢æ¨¡å¼ï¼š'train' å¼€å¯å¢å¼ºï¼Œ'val' / 'test' å…³é—­å¢å¼º"""
        assert mode in ["train", "val", "test"], f"Unsupported mode: {mode}"
        self.mode = mode

    def _augment_day(self, day):
        """æ•°æ®å¢å¼º"""
        if np.random.rand() < 0.5:
            delta = np.random.uniform(-0.5, 0.5)
            day = day + delta
        if np.random.rand() < 0.5:
            factor = np.random.uniform(0.75, 1.25)
            day = day * factor
        return day

    def __getitem__(self, idx):
        real_idx = self.valid_idx[idx]

        # ä»å†…å­˜ç¼“å­˜è¯»å–
        if self.cached_patches is not None:
            data = self.cached_patches[real_idx]
            day = data['day'].copy()
            night = data['night'].copy()
            y = data['y'].copy()
        else:
            # ä»ç£ç›˜è¯»å–ï¼ˆå¦‚æœæ²¡ç¼“å­˜ï¼‰
            row = self.gdf.iloc[real_idx]
            geom = row.geometry
            if geom.geom_type != "Point":
                geom = geom.centroid
            x, y_coord = geom.x, geom.y
            r, c = rasterio.transform.rowcol(self.transform, x, y_coord)

            half = self.patch // 2
            window = rasterio.windows.Window(c - half, r - half, self.patch, self.patch)

            day_stack = []
            for src in self.day_srcs:
                arr = src.read(1, window=window, boundless=True, masked=True)
                day_stack.append(arr.filled(0).astype(np.float32))
            day = np.stack(day_stack, axis=0)

            night_arr = self.night_src.read(1, window=window, boundless=True, masked=True)
            night = night_arr.filled(0).astype(np.float32)[np.newaxis, :, :]

            vals = []
            for f in self.target_fields:
                v = row[f]
                if v is None or (isinstance(v, float) and np.isnan(v)):
                    v = 0.0
                vals.append(float(v))
            y = np.array(vals, dtype=np.float32)
            s = y.sum()
            if s > 1e-6:
                y = y / s
            else:
                y = np.array([1.0] + [0.0] * (len(self.target_fields) - 1), dtype=np.float32)

        # æ•°æ®å¢å¼ºï¼ˆåªåœ¨è®­ç»ƒæ¨¡å¼ï¼‰
        if self.mode == 'train':
            if np.random.rand() < 0.5:
                day = np.flip(day, axis=1).copy()
                night = np.flip(night, axis=1).copy()
            if np.random.rand() < 0.5:
                day = np.flip(day, axis=2).copy()
                night = np.flip(night, axis=2).copy()
            k = np.random.randint(0, 4)
            if k > 0:
                day = np.rot90(day, k, axes=(1, 2)).copy()
                night = np.rot90(night, k, axes=(1, 2)).copy()
            day = self._augment_day(day)

        return {
            "day": torch.from_numpy(day),
            "night": torch.from_numpy(night),
            "y": torch.from_numpy(y),
        }

    def close(self):
        """å…³é—­æ–‡ä»¶å¥æŸ„"""
        if self.day_srcs is not None:
            for s in self.day_srcs:
                if not s.closed:
                    s.close()
        if self.night_src is not None and not self.night_src.closed:
            self.night_src.close()

print("âœ… PatchPointDatasetï¼ˆå†…å­˜ç¼“å­˜ç‰ˆï¼‰å®šä¹‰å®Œæˆ")




# In[ ]:


# %% [markdown]ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹å®šä¹‰
# ## ç¬¬äºŒéƒ¨åˆ†ï¼šæ¨¡å‹å®šä¹‰

# %% ResNetæ¨¡å—
class PreActBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.bn1 = nn.BatchNorm2d(in_channels)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                               stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu2 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, 
                               stride=1, padding=1, bias=False)

        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Conv2d(in_channels, out_channels, 
                                     kernel_size=1, stride=stride, bias=False)
        else:
            self.shortcut = None

    def forward(self, x):
        out = self.bn1(x)
        out = self.relu1(out)
        shortcut = self.shortcut(out) if self.shortcut is not None else x
        out = self.conv1(out)
        out = self.bn2(out)
        out = self.relu2(out)
        out = self.conv2(out)
        return out + shortcut


class ResNetStem(nn.Module):
    def __init__(self, in_channels, base_channels=64, num_blocks=5):
        super().__init__()
        self.conv_in = nn.Conv2d(in_channels, base_channels, kernel_size=3, 
                                stride=1, padding=1, bias=False)
        self.blocks = nn.Sequential(
            *[PreActBlock(base_channels, base_channels, stride=1) 
              for _ in range(num_blocks)]
        )
        self.bn_out = nn.BatchNorm2d(base_channels)
        self.relu_out = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv_in(x)
        x = self.blocks(x)
        x = self.bn_out(x)
        x = self.relu_out(x)
        return x


class FusionResNetDirichlet(nn.Module):
    def __init__(self, day_channels=7, night_channels=1, n_comp=4,
                 base_channels=64, day_blocks=3, night_blocks=3, shared_blocks=5):
        super().__init__()
        self.day_stem = ResNetStem(day_channels, base_channels, day_blocks)
        self.night_stem = ResNetStem(night_channels, base_channels, night_blocks)

        self.shared_in_conv = nn.Conv2d(2 * base_channels, base_channels, 
                                       kernel_size=3, stride=1, padding=1, bias=False)
        self.shared_blocks = nn.Sequential(
            *[PreActBlock(base_channels, base_channels, stride=1) 
              for _ in range(shared_blocks)]
        )
        self.shared_bn = nn.BatchNorm2d(base_channels)
        self.shared_relu = nn.ReLU(inplace=True)

        self.head_conv = nn.Conv2d(base_channels, base_channels, 
                                   kernel_size=3, stride=1, padding=1, bias=False)
        self.head_bn = nn.BatchNorm2d(base_channels)
        self.head_relu = nn.ReLU(inplace=True)
        self.head_drop = nn.Dropout2d(0.1)
        self.head_out = nn.Conv2d(base_channels, n_comp, kernel_size=1, bias=True)

    def forward(self, day, night):
        d = self.day_stem(day)
        n = self.night_stem(night)
        x = torch.cat([d, n], dim=1)
        x = self.shared_in_conv(x)
        x = self.shared_blocks(x)
        x = self.shared_bn(x)
        x = self.shared_relu(x)
        x = self.head_conv(x)
        x = self.head_bn(x)
        x = self.head_relu(x)
        x = self.head_drop(x)
        alpha_raw = self.head_out(x)
        alpha = F.softplus(alpha_raw) + 1.0
        return alpha

print("âœ… æ¨¡å‹å®šä¹‰å®Œæˆ")

# %% æŸå¤±å‡½æ•°
def dirichlet_nll(y_true, alpha, eps=1e-7):
    if y_true.dim() == 2:
        pass
    elif y_true.dim() > 2:
        B, C, H, W = y_true.shape
        yc = y_true[:, :, H // 2, W // 2]
        y_true = yc

    B, C, H, W = alpha.shape
    ac = alpha[:, :, H // 2, W // 2]

    alpha0 = ac.sum(dim=1, keepdim=True)
    logC = torch.lgamma(alpha0) - torch.lgamma(ac).sum(dim=1, keepdim=True)

    y_safe = torch.clamp(y_true, min=eps, max=1.0 - eps)
    logL = logC + ((ac - 1.0) * torch.log(y_safe)).sum(dim=1, keepdim=True)
    return -logL.mean()


def r2_score_numpy(y_true, y_pred):
    mask = np.isfinite(y_true).all(axis=1) & np.isfinite(y_pred).all(axis=1)
    y_true = y_true[mask]
    y_pred = y_pred[mask]
    if y_true.shape[0] < 2:
        return np.nan, [np.nan] * y_true.shape[1]

    r2_each = [r2_score(y_true[:, i], y_pred[:, i]) 
               for i in range(y_true.shape[1])]
    r2_mean = float(np.mean(r2_each))
    return r2_mean, r2_each

print("âœ… æŸå¤±å‡½æ•°å®šä¹‰å®Œæˆ")





# In[ ]:


# %% [markdown]ç¬¬ä¸‰éƒ¨åˆ†ï¼šé…ç½®å‚æ•°
# ## ç¬¬ä¸‰éƒ¨åˆ†ï¼šé…ç½®å‚æ•°

# %% é…ç½®
DAY_TIFS = [
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_RED_2020_90m_zscore.tif",
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_GREEN_2020_90m_zscore.tif",
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_BLUE_2020_90m_zscore.tif",
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_NIR_2020_90m_zscore.tif",
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_SWIR1_2020_90m_zscore.tif",
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_SWIR2_2020_90m_zscore.tif",
    r"F:\Landsat_NL_Mector_90m_zscore\Landsat_TEMP1_2020_90m_zscore.tif",
]
NIGHT_TIF = r"F:\Landsat_NL_Mector_90m_zscore\VIIRS_2020_90m_zscore.tif"
LABEL_SHP = r"F:\sample_2020\Sample_2020.shp"
TARGET_FIELDS = ["F", "F_NF", "NF_F", "NF"]

OUT_DIR = r"F:\model_outputs_2020_resnet_optimized"
os.makedirs(OUT_DIR, exist_ok=True)

# å¿«é€Ÿæµ‹è¯•æ¨¡å¼å¼€å…³
QUICK_TEST = False  # æ”¹ä¸º False è¿›è¡Œå®Œæ•´è®­ç»ƒ   True

if QUICK_TEST:
    print("\n" + "="*70)
    print("âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    print("="*70)
    MAX_SAMPLES = 500
    PATCH_SIZE = 64
    BATCH_SIZE = 32
    EPOCHS = 20
    PATIENCE = 5
    BASE_LR_LIST = [1e-3]
    WD_LIST = [1e-2]
    FRACTIONS = [0.1, 1.0]
    N_REPEATS = 1
else:
    print("\n" + "="*70)
    print("ğŸš€ å®Œæ•´è®­ç»ƒæ¨¡å¼")
    print("="*70)
    MAX_SAMPLES = None
    PATCH_SIZE = 64
    BATCH_SIZE = 256   #64
    EPOCHS = 300
    PATIENCE = 15
    BASE_LR_LIST = [1e-3, 1e-4, 1e-5] # [1e-2, 1e-3, 1e-4, 1e-5]
    WD_LIST = [1e-1, 1e-2, 1e-4]  # [1e0, 1e-1, 1e-2, 1e-3]
    FRACTIONS = [0.05, 0.10, 0.25, 0.50, 1.00]
    N_REPEATS = 1

# â­â­ æ–°å¢ï¼šé˜¶æ®µ1ä¸“ç”¨â€œè½»é‡ç‰ˆâ€è®¾ç½®ï¼ˆæ¨èï¼‰
EPOCHS_STAGE1 = 60          # ç½‘æ ¼æœç´¢æ—¶æœ€å¤š 60 è½®
PATIENCE_STAGE1 = 8         # 8 è½®ä¸æå‡å°±æ—©åœ
FRACTION_STAGE1 = 0.3       # åªç”¨è®­ç»ƒé›†çš„ 30% è¿›è¡Œè¶…å‚æœç´¢

# ==========================================================# ==========================================================
# é¢å¤–æ§åˆ¶å¼€å…³ï¼šæ˜¯å¦è¿è¡Œé˜¶æ®µ1/é˜¶æ®µ2 & æ˜¯å¦æ–­ç‚¹ç»­è·‘
# ==========================================================# ==========================================================
RUN_STAGE1 = False   # æ˜¯å¦æ‰§è¡Œé˜¶æ®µ1ï¼ˆç½‘æ ¼æœç´¢ï¼‰ï¼Œç°åœ¨ä½ å¯ä»¥å…ˆå…³æ‰# ========================================================
RUN_STAGE2 = True    # æ˜¯å¦æ‰§è¡Œé˜¶æ®µ2ï¼ˆæ•°æ®é‡å®éªŒï¼‰            # ==========================================================

RESUME_STAGE1 = True  # True = å¦‚æœå·²æœ‰ CSVï¼ŒæŒ‰å…¶ä¸­è¿›åº¦ç»­è·‘   # ==========================================================
RESUME_STAGE2 = True  # True = å¦‚æœå·²æœ‰ CSVï¼ŒæŒ‰å…¶ä¸­è¿›åº¦ç»­è·‘   # ==========================================================

# æ˜¯å¦ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šçš„è¶…å‚æ•°ï¼ˆæ¨èå…ˆ Trueï¼Œç­‰ä½ æœ‰å®Œæ•´çš„é˜¶æ®µ1ç»“æœå†åˆ‡ Falseï¼‰# =================================================
USE_MANUAL_BEST_HPARAMS = True                             # ==========================================================
MANUAL_BEST_LR = 3e-4                                      # ==========================================================
MANUAL_BEST_WD = 1e-3                                        # =========================================================
# ==========================================================# ==========================================================
# ==========================================================# ==========================================================

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
NUM_WORKERS = 0 #min(4, os.cpu_count() or 2)  # æ¢å¤å¤šçº¿ç¨‹

print(f"âœ… é…ç½®å®Œæˆ")
print(f"   è®¾å¤‡: {DEVICE}")
print(f"   DataLoader workers: {NUM_WORKERS}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")




# In[ ]:


# %% [markdown]ç¬¬å››éƒ¨åˆ†ï¼šè®­ç»ƒå‡½æ•°
# ## ç¬¬å››éƒ¨åˆ†ï¼šè®­ç»ƒå‡½æ•°

# %% è®­ç»ƒå‡½æ•°
def train_one_setting(base_ds, train_idx, val_idx, test_idx,
                      lr, weight_decay, fraction, repeat_id, fold_id,
                      epochs=None, patience=None):
    """å•æ¬¡è®­ç»ƒå’Œè¯„ä¼°ï¼ˆå®Œå…¨åŸºäº base_ds å†…å­˜ç¼“å­˜ï¼‰"""
        # å¦‚æœæ²¡ä¼ ï¼Œå°±ç”¨å…¨å±€â€œæ­£å¼ç‰ˆâ€å‚æ•°
    if epochs is None:
        epochs = EPOCHS
    if patience is None:
        patience = PATIENCE
    # ä» train_idx ä¸­éšæœºæŠ½å– fraction æ¯”ä¾‹
    n_train = int(len(train_idx) * fraction)
    n_train = max(n_train, 1)
    rng = np.random.RandomState(seed=fold_id * 100 + repeat_id * 7 + int(fraction * 100))
    sub_train_idx = rng.choice(train_idx, size=n_train, replace=False)

    # ç”¨åŒä¸€ä¸ª base_dsï¼Œåˆ›å»ºä¸åŒçš„ Subset
    train_subset = Subset(base_ds, list(sub_train_idx))
    val_subset   = Subset(base_ds, list(val_idx))
    test_subset  = Subset(base_ds, list(test_idx))

    # DataLoaderï¼ˆå¤šçº¿ç¨‹ï¼‰
    train_loader = DataLoader(
        train_subset, batch_size=BATCH_SIZE, shuffle=True,
        num_workers=NUM_WORKERS, pin_memory=True,
        persistent_workers=True if NUM_WORKERS > 0 else False
    )
    val_loader = DataLoader(
        val_subset, batch_size=BATCH_SIZE, shuffle=False,
        num_workers=NUM_WORKERS, pin_memory=True,
        persistent_workers=True if NUM_WORKERS > 0 else False
    )
    test_loader = DataLoader(
        test_subset, batch_size=BATCH_SIZE, shuffle=False,
        num_workers=NUM_WORKERS, pin_memory=True,
        persistent_workers=True if NUM_WORKERS > 0 else False
    )

    # æ¨¡å‹
    model = FusionResNetDirichlet(
        day_channels=len(DAY_TIFS), night_channels=1,
        n_comp=len(TARGET_FIELDS),
    ).to(DEVICE)

    optimizer = torch.optim.AdamW(
        model.parameters(), lr=lr, weight_decay=weight_decay
    )

    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-7
    )

    best_val_r2 = -1e9
    best_state = None
    epochs_no_improve = 0

    pbar = tqdm(range(1, epochs + 1), desc=f"Fold{fold_id} frac={fraction:.2f}")

    for epoch in pbar:
        # ========= è®­ç»ƒ =========
        base_ds.set_mode('train')
        model.train()
        train_loss = 0.0
        for batch in train_loader:
            day = batch["day"].to(DEVICE, non_blocking=True)
            night = batch["night"].to(DEVICE, non_blocking=True)
            y = batch["y"].to(DEVICE, non_blocking=True)

            optimizer.zero_grad()
            alpha = model(day, night)
            loss = dirichlet_nll(y, alpha)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss += loss.item() * day.size(0)

        train_loss /= len(train_loader.dataset)

        # ========= éªŒè¯ =========
        base_ds.set_mode('val')
        model.eval()
        val_loss = 0.0
        y_true_all = []
        y_pred_all = []
        with torch.no_grad():
            for batch in val_loader:
                day = batch["day"].to(DEVICE, non_blocking=True)
                night = batch["night"].to(DEVICE, non_blocking=True)
                y = batch["y"].to(DEVICE, non_blocking=True)

                alpha = model(day, night)
                loss = dirichlet_nll(y, alpha)
                val_loss += loss.item() * day.size(0)

                B, C, H, W = alpha.shape
                ac = alpha[:, :, H // 2, W // 2].cpu().numpy()
                pred_comp = ac / np.clip(ac.sum(axis=1, keepdims=True), 1e-6, None)

                y_np = y.cpu().numpy()
                y_true_all.append(y_np)
                y_pred_all.append(pred_comp)

        val_loss /= len(val_loader.dataset)
        y_true_all = np.vstack(y_true_all)
        y_pred_all = np.vstack(y_pred_all)
        val_r2_mean, _ = r2_score_numpy(y_true_all, y_pred_all)

        scheduler.step(val_r2_mean)
        current_lr = optimizer.param_groups[0]['lr']

        pbar.set_postfix({
            'loss': f'{train_loss:.3f}',
            'val_r2': f'{val_r2_mean:.3f}',
            'best': f'{best_val_r2:.3f}',
            'lr': f'{current_lr:.2e}'
        })

        # æ—©åœæ—¶ç”¨ patience
        if val_r2_mean > best_val_r2 + 1e-4:
            best_val_r2 = val_r2_mean
            best_state = model.state_dict()
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                pbar.set_description(f"Fold{fold_id} frac={fraction:.2f} [æ—©åœ]")
                break

    # ========= æµ‹è¯• =========
    if best_state is not None:
        model.load_state_dict(best_state)

    base_ds.set_mode('val')
    model.eval()
    y_true_all = []
    y_pred_all = []
    with torch.no_grad():
        for batch in test_loader:
            day = batch["day"].to(DEVICE, non_blocking=True)
            night = batch["night"].to(DEVICE, non_blocking=True)
            y = batch["y"].to(DEVICE, non_blocking=True)

            alpha = model(day, night)
            B, C, H, W = alpha.shape
            ac = alpha[:, :, H // 2, W // 2].cpu().numpy()
            pred_comp = ac / np.clip(ac.sum(axis=1, keepdims=True), 1e-6, None)

            y_np = y.cpu().numpy()
            y_true_all.append(y_np)
            y_pred_all.append(pred_comp)

    y_true_all = np.vstack(y_true_all)
    y_pred_all = np.vstack(y_pred_all)
    test_r2_mean, test_r2_each = r2_score_numpy(y_true_all, y_pred_all)

    # ä¿å­˜æ¨¡å‹ï¼ˆåªåœ¨ fraction=1.0 æ—¶ï¼‰
    model_path = None
    if abs(fraction - 1.0) < 1e-6:
        model_path = os.path.join(
            OUT_DIR,
            f"model_fold{fold_id}_rep{repeat_id}_lr{lr:g}_wd{weight_decay:g}.pth",
        )
        torch.save(best_state, model_path)

    metrics = {
        "fold": fold_id, "fraction": fraction, "repeat": repeat_id,
        "lr": lr, "weight_decay": weight_decay,
        "n_train": len(sub_train_idx), "n_val": len(val_idx), "n_test": len(test_idx),
        "val_r2_best": best_val_r2, "test_r2_mean": test_r2_mean,
    }
    for name, r2v in zip(TARGET_FIELDS, test_r2_each):
        metrics[f"test_r2_{name}"] = r2v

    if model_path is not None:
        metrics["model_path"] = model_path

    return metrics

print("âœ… è®­ç»ƒå‡½æ•°å®šä¹‰å®Œæˆ")



# In[ ]:


# %% [markdown]ç¬¬äº”éƒ¨åˆ†ï¼šæ„å»ºæ•°æ®é›†
# ## ç¬¬äº”éƒ¨åˆ†ï¼šæ„å»ºæ•°æ®é›†

# %% æ„å»ºæ•°æ®é›†
print("\n" + "="*70)
print("ğŸ“Š æ„å»ºåŸºç¡€æ•°æ®é›†...")
print("="*70)

base_ds = PatchPointDataset(
    LABEL_SHP, DAY_TIFS, NIGHT_TIF, TARGET_FIELDS,
    patch_size=PATCH_SIZE, mode='val', check_valid=True, max_samples=MAX_SAMPLES
)

n_total = len(base_ds)
all_idx = np.arange(n_total)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
folds = list(kf.split(all_idx))

print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œå…±{n_total}ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼Œåˆ’åˆ†ä¸º5æŠ˜")





# In[ ]:


# # %% [markdown]ç¬¬å…­éƒ¨åˆ†ï¼šé˜¶æ®µ1 - ç½‘æ ¼æœç´¢ï¼ˆå¯é€‰ + æ–­ç‚¹ç»­è·‘ï¼‰
# # ## ç¬¬å…­éƒ¨åˆ†ï¼šé˜¶æ®µ1 - ç½‘æ ¼æœç´¢ï¼ˆå¯é€‰ + æ–­ç‚¹ç»­è·‘ï¼‰

# GRID_CSV = os.path.join(OUT_DIR, "stage1_grid_search.csv")

# if RUN_STAGE1:
#     print("\n" + "="*70)
#     print("é˜¶æ®µ1ï¼šç½‘æ ¼æœç´¢æœ€ä½³è¶…å‚æ•°ï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰")
#     print("="*70)

#     # 1. å¦‚æœé€‰æ‹©ç»­è·‘ä¸”å·²æœ‰ç»“æœæ–‡ä»¶ï¼Œå°±è¯»è¿›æ¥
#     if RESUME_STAGE1 and os.path.exists(GRID_CSV):
#         df_grid = pd.read_csv(GRID_CSV)
#         print(f"ğŸ” æ£€æµ‹åˆ°å·²æœ‰é˜¶æ®µ1ç»“æœï¼Œå°†åœ¨å…¶åŸºç¡€ä¸Šç»­è·‘ï¼š{len(df_grid)} æ¡è®°å½•")
#     else:
#         df_grid = pd.DataFrame()

#     # 2. å·²å®Œæˆçš„ (lr, wd, fold) ç»„åˆ
#     done_keys = set()
#     if not df_grid.empty and "fold" in df_grid.columns:
#         for _, row in df_grid.iterrows():
#             try:
#                 done_keys.add((float(row["lr"]), float(row["weight_decay"]), int(row["fold"])))
#             except Exception:
#                 continue

#     new_results = []

#     # 3. æ­£å¼ç½‘æ ¼æœç´¢
#     for lr in BASE_LR_LIST:
#         for wd in WD_LIST:
#             print(f"\n--- lr={lr:g}, wd={wd:g} ---")

#             for fold_id, (trainval_idx, test_idx) in enumerate(folds, start=1):
#                 key = (float(lr), float(wd), int(fold_id))
#                 if key in done_keys:
#                     print(f"â­ï¸ è·³è¿‡å·²å®Œæˆï¼šlr={lr:g}, wd={wd:g}, fold={fold_id}")
#                     continue

#                 rng = np.random.RandomState(seed=fold_id)
#                 rng.shuffle(trainval_idx)
#                 n_val = max(1, len(trainval_idx) // 4)
#                 val_idx = trainval_idx[:n_val]
#                 train_idx = trainval_idx[n_val:]

#                 try:
#                     m = train_one_setting(
#                         base_ds, train_idx, val_idx, test_idx,
#                         lr, wd, 1.0, 0, fold_id,
#                     )
#                     # ç¡®ä¿ lr / wd å†™å…¥ç»“æœï¼ˆtrain_one_setting é‡Œæ²¡å†™å°±è¡¥ä¸€ä¸‹ï¼‰
#                     m["lr"] = lr
#                     m["weight_decay"] = wd
#                     new_results.append(m)

#                     # æ¯æ¬¡æ›´æ–°åç«‹åˆ»å†™ç›˜ï¼Œé˜²æ­¢åœç”µä¸¢è¿›åº¦
#                     df_all = pd.concat([df_grid, pd.DataFrame(new_results)], ignore_index=True)
#                     df_all.to_csv(GRID_CSV, index=False)

#                 except Exception as e:
#                     print(f"   âŒ Fold{fold_id} å‡ºé”™: {e}")
#                     continue

#     # 4. æœ€ç»ˆåˆå¹¶ä¿å­˜
#     if new_results:
#         df_grid = pd.concat([df_grid, pd.DataFrame(new_results)], ignore_index=True)
#         df_grid.to_csv(GRID_CSV, index=False)
#         print(f"\nâœ… é˜¶æ®µ1å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {GRID_CSV}")
#     else:
#         print("\nâ„¹ï¸ é˜¶æ®µ1æ²¡æœ‰æ–°å¢ç»“æœï¼ˆå¯èƒ½å…¨éƒ¨å·²å®Œæˆï¼‰")
# else:
#     print("\n[é˜¶æ®µ1] å·²è·³è¿‡ç½‘æ ¼æœç´¢ï¼ˆRUN_STAGE1 = Falseï¼‰")


# In[ ]:


# åœ¨é˜¶æ®µ2å¼€å§‹å‰ï¼Œç¡®å®šè¦ä½¿ç”¨çš„è¶…å‚æ•° best_lr / best_wd
# ==========================================================
if USE_MANUAL_BEST_HPARAMS:
    best_lr = MANUAL_BEST_LR
    best_wd = MANUAL_BEST_WD
    print(f"\nâš ï¸ é˜¶æ®µ2ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šè¶…å‚æ•°: lr={best_lr:g}, wd={best_wd:g}")
else:
    # å°è¯•ä»é˜¶æ®µ1ç»“æœä¸­è‡ªåŠ¨é€‰å‡ºæœ€ä¼˜ (lr, wd)
    GRID_CSV = os.path.join(OUT_DIR, "stage1_grid_search.csv")
    if os.path.exists(GRID_CSV):
        df_grid = pd.read_csv(GRID_CSV)
        if "error" in df_grid.columns:
            df_ok = df_grid[df_grid["error"].isna()]
        else:
            df_ok = df_grid

        if not df_ok.empty:
            best_config = df_ok.groupby(["lr", "weight_decay"])["test_r2_mean"].mean().idxmax()
            best_lr, best_wd = best_config
            print(f"\nğŸ† ä»é˜¶æ®µ1ç»“æœä¸­è¯»å–è¶…å‚æ•°: lr={best_lr:g}, wd={best_wd:g}")
        else:
            best_lr, best_wd = MANUAL_BEST_LR, MANUAL_BEST_WD
            print(f"\nâš ï¸ é˜¶æ®µ1ç»“æœä¸ºç©ºï¼Œå›é€€åˆ°æ‰‹åŠ¨è¶…å‚æ•°: lr={best_lr:g}, wd={best_wd:g}")
    else:
        best_lr, best_wd = MANUAL_BEST_LR, MANUAL_BEST_WD
        print(f"\nâš ï¸ æœªæ‰¾åˆ°é˜¶æ®µ1ç»“æœæ–‡ä»¶ï¼Œå›é€€åˆ°æ‰‹åŠ¨è¶…å‚æ•°: lr={best_lr:g}, wd={best_wd:g}")


# In[ ]:


# %% [markdown]ç¬¬ä¸ƒéƒ¨åˆ†ï¼šé˜¶æ®µ2 - æ•°æ®é‡æµ‹è¯•ï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
# ## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šé˜¶æ®µ2 - æ•°æ®é‡æµ‹è¯•ï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰

FINAL_CSV = os.path.join(OUT_DIR, "stage2_fraction_results.csv")

print("\n" + "="*70)
print(f"é˜¶æ®µ2ï¼šæµ‹è¯•æ•°æ®é‡å½±å“ï¼ˆlr={best_lr:g}, wd={best_wd:g}ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰")
print("="*70)

# 1. å¦‚æœå·²ç»æœ‰é˜¶æ®µ2ç»“æœï¼ˆä¸Šæ¬¡è·‘äº†ä¸€éƒ¨åˆ†ï¼‰ï¼Œå°±è¯»è¿›æ¥
if os.path.exists(FINAL_CSV):
    df_final = pd.read_csv(FINAL_CSV)
    print(f"ğŸ” æ£€æµ‹åˆ°å·²æœ‰é˜¶æ®µ2ç»“æœï¼Œå°†åœ¨å…¶åŸºç¡€ä¸Šç»­è·‘ï¼š{len(df_final)} æ¡è®°å½•")
else:
    df_final = pd.DataFrame()

# 2. æŠŠå·²ç»å®Œæˆçš„ (fraction, repeat, fold) ç»„åˆè®°ä¸‹æ¥ï¼Œé¿å…é‡å¤è·‘
done_keys = set()
if not df_final.empty:
    if "error" in df_final.columns:
        df_ok = df_final[df_final["error"].isna()]
    else:
        df_ok = df_final

    for _, row in df_ok.iterrows():
        try:
            frac_done = float(row["fraction"])
            rep_done = int(row["repeat"])
            fold_done = int(row["fold"])
            done_keys.add((frac_done, rep_done, fold_done))
        except Exception:
            continue

all_results = df_final.to_dict("records") if not df_final.empty else []

# 3. æ­£å¼è·‘ FRACTIONS Ã— N_REPEATS Ã— 5-fold
for frac in FRACTIONS:
    print(f"\n>>> Fraction = {frac*100:.0f}%")
    for rep in range(N_REPEATS):
        for fold_id, (trainval_idx, test_idx) in enumerate(folds, start=1):
            key = (float(frac), int(rep), int(fold_id))
            if key in done_keys:
                print(f"â­ï¸ è·³è¿‡å·²å®Œæˆï¼šfrac={frac:.2f}, rep={rep}, fold={fold_id}")
                continue

            rng = np.random.RandomState(seed=fold_id)
            rng.shuffle(trainval_idx)
            n_val = max(1, len(trainval_idx) // 4)
            val_idx = trainval_idx[:n_val]
            train_idx = trainval_idx[n_val:]

            try:
                m = train_one_setting(
                    base_ds, train_idx, val_idx, test_idx,
                    best_lr, best_wd, frac, rep, fold_id,
                )
                all_results.append(m)

                # ğŸ”¥ æ¯å®Œæˆä¸€ä¸ªç»„åˆå°±ç«‹åˆ»å†™ç›˜ï¼Œé˜²æ­¢åœç”µé‡æ¥
                pd.DataFrame(all_results).to_csv(FINAL_CSV, index=False)

            except Exception as e:
                print(f"   âŒ Fold{fold_id}/rep{rep}/frac{frac:.2f} å‡ºé”™: {e}")
                all_results.append({
                    "fold": fold_id,
                    "fraction": frac,
                    "repeat": rep,
                    "lr": best_lr,
                    "weight_decay": best_wd,
                    "error": str(e),
                })
                pd.DataFrame(all_results).to_csv(FINAL_CSV, index=False)
                continue

# 4. æœ€ç»ˆä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢ä¸‡ä¸€
df_final = pd.DataFrame(all_results)
df_final.to_csv(FINAL_CSV, index=False)

print("\n" + "="*70)
print("âœ… é˜¶æ®µ2è®­ç»ƒå®Œæˆï¼")
print(f"   é˜¶æ®µ2ç»“æœ: {FINAL_CSV}")
print("="*70)

# æ¸…ç†
base_ds.close()



# In[ ]:


# %% [markdown]ç¬¬å…«éƒ¨åˆ†ï¼šå¿«é€Ÿæ€»ç»“ï¼ˆå…¼å®¹â€œé˜¶æ®µ1å·²æ³¨é‡Šâ€çš„æƒ…å†µï¼‰
# ## ç¬¬å…«éƒ¨åˆ†ï¼šå¿«é€Ÿæ€»ç»“ï¼ˆå…¼å®¹â€œé˜¶æ®µ1å·²æ³¨é‡Šâ€çš„æƒ…å†µï¼‰

print("\n" + "="*70)
print("ğŸ“Š è®­ç»ƒæ€»ç»“")
print("="*70)

# å°è¯•è¯»å–é˜¶æ®µ1ç»“æœï¼ˆå¦‚æœä½ ä»¥åæ¢å¤é˜¶æ®µ1ï¼Œä»ç„¶èƒ½ç”¨ï¼‰
GRID_CSV = os.path.join(OUT_DIR, "stage1_grid_search.csv")
if os.path.exists(GRID_CSV):
    df_grid = pd.read_csv(GRID_CSV)
    if not df_grid.empty:
        if "error" in df_grid.columns:
            df_grid_ok = df_grid[df_grid["error"].isna()]
        else:
            df_grid_ok = df_grid

        if not df_grid_ok.empty:
            print("\nã€é˜¶æ®µ1ï¼šç½‘æ ¼æœç´¢ã€‘")
            print(f"  æµ‹è¯•è¶…å‚ç»„åˆ: {len(df_grid_ok.groupby(['lr', 'weight_decay']))}")
            print(f"  æ€»å®éªŒæ¬¡æ•°: {len(df_grid_ok)}")
            # è¿™é‡Œä¸å†å¼ºè¡Œæ‰“å° best_r2ï¼ˆå› ä¸ºä½ å¯èƒ½ç”¨äº†æ‰‹åŠ¨è¶…å‚ï¼‰
            best_row = df_grid_ok.loc[df_grid_ok['test_r2_mean'].idxmax()]
            print(f"  æœ€ä½³è¶…å‚: lr={best_row['lr']:.0e}, wd={best_row['weight_decay']:.0e}")
            print(f"  å¯¹åº”æµ‹è¯•RÂ²: {best_row['test_r2_mean']:.4f}")
else:
    print("\nã€é˜¶æ®µ1ï¼šç½‘æ ¼æœç´¢ã€‘")
    print("  å·²è·³è¿‡æˆ–å°šæœªè¿è¡Œï¼ˆæœªæ‰¾åˆ° stage1_grid_search.csvï¼‰")

# é˜¶æ®µ2æ±‡æ€»
if os.path.exists(FINAL_CSV):
    df_final = pd.read_csv(FINAL_CSV)
    if not df_final.empty:
        df_clean = df_final[df_final['error'].isna()] if 'error' in df_final.columns else df_final
        if not df_clean.empty:
            print("\nã€é˜¶æ®µ2ï¼šæ•°æ®é‡æµ‹è¯•ã€‘")
            print(f"  æµ‹è¯•æ¯”ä¾‹: {sorted(df_clean['fraction'].unique())}")
            print(f"  æ€»å®éªŒæ¬¡æ•°: {len(df_clean)}")
            print("\n  å„æ•°æ®æ¯”ä¾‹æ€§èƒ½:")
            for frac in sorted(df_clean['fraction'].unique()):
                subset = df_clean[df_clean['fraction'] == frac]
                mean_r2 = subset['test_r2_mean'].mean()
                std_r2 = subset['test_r2_mean'].std()
                print(f"    {frac*100:5.1f}%: RÂ²={mean_r2:.4f}Â±{std_r2:.4f}")

print("\n" + "="*70)

