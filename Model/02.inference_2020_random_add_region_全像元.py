"""
å…¨å›¾æ¨ç†è„šæœ¬ - åŒºåŸŸè‡ªé€‚åº”é›†æˆç‰ˆï¼ˆå†…å­˜å®‰å…¨ç‰ˆï¼‰
================================================================

ä¼˜åŒ–ç‰¹æ€§:
1. âœ… åŒºåŸŸè‡ªé€‚åº”é›†æˆï¼šæ ¹æ®åœ°ç†ä½ç½®åŠ¨æ€è°ƒæ•´æ¨¡å‹æƒé‡
2. âœ… ç²¾ç¡®é¢„ç­›é€‰ï¼šåŸå§‹åˆ†è¾¨ç‡ç­›é€‰ï¼Œ100%å‡†ç¡®
3. âœ… DataLoaderå¤šè¿›ç¨‹ï¼šä»ç£ç›˜é«˜æ•ˆè¯»å–ï¼ˆä¸é¢„åŠ è½½åˆ°å†…å­˜ï¼‰
4. âœ… ä¸¥æ ¼å†…å­˜ç®¡ç†ï¼šæ¨ç†åç«‹å³é‡Šæ”¾ï¼Œé¿å…å½’ä¸€åŒ–æ—¶OOM
5. âœ… NoDataæ­£ç¡®å¤„ç†ï¼šåªå¯¹æœ‰æ•ˆåƒå…ƒé¢„æµ‹
6. âœ… æ–­ç‚¹ç»­ä¼ ã€æ··åˆç²¾åº¦ç­‰æ‰€æœ‰ä¼˜åŒ–ä¿ç•™

å†…å­˜ç­–ç•¥å˜æ›´:
- åŸç‰ˆï¼šé¢„åŠ è½½æ•°æ®åˆ°å†…å­˜ï¼ˆ~90GBï¼‰âŒ å†…å­˜ä¸è¶³
- æ–°ç‰ˆï¼šDataLoaderå¤šè¿›ç¨‹è¯»å–ï¼ˆå³°å€¼~15GBï¼‰âœ… å®‰å…¨

ä½œè€…ï¼šClaude & Dawa
æ—¥æœŸï¼š2025-11-30
ç‰ˆæœ¬ï¼šv4.0 - å†…å­˜å®‰å…¨ç‰ˆ
"""

# %% å¯¼å…¥åº“
import os
import sys
import glob
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast
import rasterio
from rasterio.windows import Window
from tqdm import tqdm
import warnings
import time
import json
import geopandas as gpd
from shapely.geometry import Point
import gc
warnings.filterwarnings('ignore')

print("=" * 80)
print("ğŸ—ºï¸  å…¨å›¾æ¨ç† - åŒºåŸŸè‡ªé€‚åº”é›†æˆç‰ˆï¼ˆå†…å­˜å®‰å…¨ç‰ˆï¼‰")
print("=" * 80)

# %% ============================================================================
#                           ç¬¬ä¸€éƒ¨åˆ†ï¼šé…ç½®å‚æ•°
# ==============================================================================

print("\n[1] é…ç½®å‚æ•°...")

# ========== è·¯å¾„é…ç½® ==========
DATA_DIR = r"F:\Landsat_NL_Mector_90m_zscore"
MODEL_DIR = r"F:\model_outputs_2020_resnet_optimized"
OOR_MODEL_DIR = r"F:\model_outputs_2020_OUT_OF_REGION_MACRO_SOFT"
PROVINCE_SHP = r"F:\Province_boundary\Provinces_China.shp"
OUT_DIR = os.path.join(MODEL_DIR, "maps_2020_ensemble_regional_adaptive")

os.makedirs(OUT_DIR, exist_ok=True)

# Randomä¸»æ¨¡å‹
RANDOM_MODEL_PATH = os.path.join(MODEL_DIR, "model_fold3_rep0_lr0.0003_wd0.001.pth")

# 6ä¸ªOORåŒºåŸŸæ¨¡å‹
OOR_MODELS = {
    "ä¸œåŒ—": os.path.join(OOR_MODEL_DIR, "model_OOR_macro_soft_fold1_lr0.0003_wd0.001.pth"),
    "ååŒ—": os.path.join(OOR_MODEL_DIR, "model_OOR_macro_soft_fold2_lr0.0003_wd0.001.pth"),
    "åä¸œ": os.path.join(OOR_MODEL_DIR, "model_OOR_macro_soft_fold3_lr0.0003_wd0.001.pth"),
    "ä¸­å—": os.path.join(OOR_MODEL_DIR, "model_OOR_macro_soft_fold4_lr0.0003_wd0.001.pth"),
    "è¥¿å—": os.path.join(OOR_MODEL_DIR, "model_OOR_macro_soft_fold5_lr0.0003_wd0.001.pth"),
    "è¥¿åŒ—": os.path.join(OOR_MODEL_DIR, "model_OOR_macro_soft_fold6_lr0.0003_wd0.001.pth"),
}

# è¾“å…¥å½±åƒ
DAY_BANDS = ["RED", "GREEN", "BLUE", "NIR", "SWIR1", "SWIR2", "TEMP1"]
DAY_TIFS = [
    os.path.join(DATA_DIR, f"Landsat_{band}_2020_90m_zscore.tif")
    for band in DAY_BANDS
]
NIGHT_TIF = os.path.join(DATA_DIR, "VIIRS_2020_90m_zscore.tif")

# æ¨ç†å‚æ•°
PATCH_SIZE = 64
STEP = 32
TARGET_FIELDS = ["F", "F_NF", "NF_F", "NF"]

# ä¼˜åŒ–å‚æ•°ï¼ˆé’ˆå¯¹RTX A5000 24GBï¼‰
BATCH_SIZE = 1024
NUM_WORKERS = 0  # Windowså…¼å®¹æ€§ï¼šè®¾ä¸º0é¿å…multiprocessingé—®é¢˜
PREFETCH_FACTOR = 2
USE_AMP = True
PIN_MEMORY = True

# æ£€æŸ¥ç‚¹è®¾ç½®
CHECKPOINT_INTERVAL = 1000
AUTO_SAVE = True

# ========== åŒºåŸŸè‡ªé€‚åº”æƒé‡é…ç½® ==========
REGION_WEIGHTS = {
    "è¥¿åŒ—": {'random': 0.40, 'oor': 0.60},
    "åä¸œ": {'random': 0.50, 'oor': 0.50},
    "ä¸­å—": {'random': 0.65, 'oor': 0.35},
    "ä¸œåŒ—": {'random': 0.80, 'oor': 0.20},
    "è¥¿å—": {'random': 0.80, 'oor': 0.20},
    "ååŒ—": {'random': 1.00, 'oor': 0.00},
}

# å®åŒºå®šä¹‰
MACRO_REGION_DEF = {
    "ä¸œåŒ—": ["è¾½å®çœ", "å‰æ—çœ", "é»‘é¾™æ±Ÿçœ"],
    "ååŒ—": ["åŒ—äº¬å¸‚", "å¤©æ´¥å¸‚", "æ²³åŒ—çœ", "å±±è¥¿çœ", "å†…è’™å¤è‡ªæ²»åŒº"],
    "åä¸œ": ["ä¸Šæµ·å¸‚", "æ±Ÿè‹çœ", "æµ™æ±Ÿçœ", "å®‰å¾½çœ", "ç¦å»ºçœ", "æ±Ÿè¥¿çœ", "å±±ä¸œçœ"],
    "ä¸­å—": ["æ²³å—çœ", "æ¹–åŒ—çœ", "æ¹–å—çœ", "å¹¿ä¸œçœ", "å¹¿è¥¿å£®æ—è‡ªæ²»åŒº", "æµ·å—çœ"],
    "è¥¿å—": ["é‡åº†å¸‚", "å››å·çœ", "è´µå·çœ", "äº‘å—çœ"],
    "è¥¿åŒ—": ["é™•è¥¿çœ", "ç”˜è‚ƒçœ", "é’æµ·çœ", "å®å¤å›æ—è‡ªæ²»åŒº", "æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº"],
}

print(f"   GPU: {torch.cuda.get_device_name(0)}")
print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
print(f"   é›†æˆç­–ç•¥: åŒºåŸŸè‡ªé€‚åº”")
print(f"   æ•°æ®è¯»å–: DataLoaderå¤šè¿›ç¨‹ï¼ˆä¸é¢„åŠ è½½ï¼‰")
print(f"   å†…å­˜ä¼˜åŒ–: å¯ç”¨ï¼ˆæ¨ç†åé‡Šæ”¾ï¼‰")


# %% ============================================================================
#                           ç¬¬äºŒéƒ¨åˆ†ï¼šè·¯å¾„æ£€æŸ¥
# ==============================================================================

print("\n[2] æ£€æŸ¥æ–‡ä»¶è·¯å¾„...")

if not os.path.exists(PROVINCE_SHP):
    print(f"âŒ çœç•Œæ–‡ä»¶ä¸å­˜åœ¨: {PROVINCE_SHP}")
    sys.exit(1)

if not os.path.exists(RANDOM_MODEL_PATH):
    print(f"âŒ Randomæ¨¡å‹ä¸å­˜åœ¨: {RANDOM_MODEL_PATH}")
    sys.exit(1)

missing_oor = []
for region, path in OOR_MODELS.items():
    if not os.path.exists(path):
        missing_oor.append(region)

if missing_oor:
    print(f"âŒ ä»¥ä¸‹OORæ¨¡å‹ä¸å­˜åœ¨: {', '.join(missing_oor)}")
    sys.exit(1)

missing_files = []
for i, path in enumerate(DAY_TIFS, 1):
    if not os.path.exists(path):
        missing_files.append(f"æ—¥é—´å½±åƒ{i}")

if not os.path.exists(NIGHT_TIF):
    missing_files.append("å¤œå…‰å½±åƒ")

if missing_files:
    print("âŒ ä»¥ä¸‹æ–‡ä»¶ä¸å­˜åœ¨:")
    for mf in missing_files:
        print(f"   {mf}")
    sys.exit(1)

print("âœ… æ‰€æœ‰æ–‡ä»¶æ£€æŸ¥é€šè¿‡")


# %% ============================================================================
#                           ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¨¡å‹å®šä¹‰
# ==============================================================================

print("\n[3] å®šä¹‰æ¨¡å‹æ¶æ„...")

class PreActBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.bn1 = nn.BatchNorm2d(in_channels)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,
                               stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu2 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                               stride=1, padding=1, bias=False)

        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Conv2d(in_channels, out_channels,
                                     kernel_size=1, stride=stride, bias=False)
        else:
            self.shortcut = None

    def forward(self, x):
        out = self.bn1(x)
        out = self.relu1(out)
        shortcut = self.shortcut(out) if self.shortcut is not None else x
        out = self.conv1(out)
        out = self.bn2(out)
        out = self.relu2(out)
        out = self.conv2(out)
        return out + shortcut


class ResNetStem(nn.Module):
    def __init__(self, in_channels, base_channels=64, num_blocks=5):
        super().__init__()
        self.conv_in = nn.Conv2d(in_channels, base_channels, kernel_size=3,
                                stride=1, padding=1, bias=False)
        self.blocks = nn.Sequential(
            *[PreActBlock(base_channels, base_channels, stride=1)
              for _ in range(num_blocks)]
        )
        self.bn_out = nn.BatchNorm2d(base_channels)
        self.relu_out = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv_in(x)
        x = self.blocks(x)
        x = self.bn_out(x)
        x = self.relu_out(x)
        return x


class FusionResNetDirichlet(nn.Module):
    def __init__(self, day_channels=7, night_channels=1, n_comp=4,
                 base_channels=64, day_blocks=3, night_blocks=3, shared_blocks=5):
        super().__init__()
        self.day_stem = ResNetStem(day_channels, base_channels, day_blocks)
        self.night_stem = ResNetStem(night_channels, base_channels, night_blocks)

        self.shared_in_conv = nn.Conv2d(2 * base_channels, base_channels,
                                       kernel_size=3, stride=1, padding=1, bias=False)
        self.shared_blocks = nn.Sequential(
            *[PreActBlock(base_channels, base_channels, stride=1)
              for _ in range(shared_blocks)]
        )
        self.shared_bn = nn.BatchNorm2d(base_channels)
        self.shared_relu = nn.ReLU(inplace=True)

        self.head_conv = nn.Conv2d(base_channels, base_channels,
                                   kernel_size=3, stride=1, padding=1, bias=False)
        self.head_bn = nn.BatchNorm2d(base_channels)
        self.head_relu = nn.ReLU(inplace=True)
        self.head_drop = nn.Dropout2d(0.1)
        self.head_out = nn.Conv2d(base_channels, n_comp, kernel_size=1, bias=True)

    def forward(self, day, night):
        d = self.day_stem(day)
        n = self.night_stem(night)
        
        x = torch.cat([d, n], dim=1)
        x = self.shared_in_conv(x)
        x = self.shared_blocks(x)
        x = self.shared_bn(x)
        x = self.shared_relu(x)
        
        x = self.head_conv(x)
        x = self.head_bn(x)
        x = self.head_relu(x)
        x = self.head_drop(x)
        alpha_raw = self.head_out(x)
        
        alpha = F.softplus(alpha_raw) + 1.0
        
        return alpha

print("âœ… æ¨¡å‹æ¶æ„å®šä¹‰å®Œæˆ")


# %% ============================================================================
#                           ç¬¬å››éƒ¨åˆ†ï¼šåŒºåŸŸåŒ¹é…ç³»ç»Ÿ
# ==============================================================================

class RegionMatcher:
    """åŒºåŸŸåŒ¹é…å™¨ - æ ¹æ®ç»çº¬åº¦ç¡®å®šæ‰€å±å®åŒº"""
    
    def __init__(self, province_shp_path):
        print("\nğŸ—ºï¸  åˆå§‹åŒ–åŒºåŸŸåŒ¹é…ç³»ç»Ÿ...")
        
        self.provinces_gdf = gpd.read_file(province_shp_path)
        
        exclude_regions = ['é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒº', 'æ¾³é—¨ç‰¹åˆ«è¡Œæ”¿åŒº', 'å°æ¹¾çœ']
        self.provinces_gdf = self.provinces_gdf[
            ~self.provinces_gdf['çœ'].isin(exclude_regions)
        ]
        
        print(f"   æœ‰æ•ˆçœä»½: {len(self.provinces_gdf)}")
        
        self.province_to_macro = {}
        for macro_name, provinces in MACRO_REGION_DEF.items():
            for prov in provinces:
                self.province_to_macro[prov] = macro_name
        
        print(f"   å®åŒºæ•°é‡: {len(MACRO_REGION_DEF)}")
        for macro_name in MACRO_REGION_DEF:
            count = sum(1 for p in self.province_to_macro.values() if p == macro_name)
            print(f"      {macro_name}: {count}ä¸ªçœä»½")
        
        print("   âœ… åŒºåŸŸåŒ¹é…ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def get_region(self, lon, lat):
        """æ ¹æ®ç»çº¬åº¦è·å–æ‰€å±å®åŒº"""
        point = Point(lon, lat)
        matches = self.provinces_gdf[self.provinces_gdf.contains(point)]
        
        if len(matches) == 0:
            return None
        
        province_name = matches.iloc[0]['çœ']
        macro_region = self.province_to_macro.get(province_name, None)
        
        return macro_region
    
    def create_region_raster(self, reference_tif, output_path=None):
        """åˆ›å»ºåŒºåŸŸæ …æ ¼ï¼ˆåªå¯¹æœ‰æ•ˆåƒå…ƒèµ‹å€¼ï¼‰"""
        print("\n   ğŸ”§ ç”ŸæˆåŒºåŸŸæ …æ ¼ï¼ˆåŸå§‹åˆ†è¾¨ç‡ï¼Œ100%å‡†ç¡®ï¼‰...")
        
        with rasterio.open(reference_tif) as src:
            height = src.height
            width = src.width
            transform = src.transform
            
            print(f"      æ …æ ¼å°ºå¯¸: {height:,} Ã— {width:,}")
            
            print(f"      è¯»å–æ•°æ®æ©è†œï¼ˆåŸå§‹åˆ†è¾¨ç‡ï¼‰...")
            data = src.read(1, masked=True)
            valid_mask = ~data.mask
            
            valid_count = valid_mask.sum()
            total_count = height * width
            valid_percent = valid_count / total_count * 100
            
            print(f"      æœ‰æ•ˆåƒå…ƒ: {valid_count:,} / {total_count:,} ({valid_percent:.1f}%)")
            print(f"      NoDataåƒå…ƒ: {total_count - valid_count:,} ({100-valid_percent:.1f}%)")
            
            region_codes = {
                "ä¸œåŒ—": 1, "ååŒ—": 2, "åä¸œ": 3,
                "ä¸­å—": 4, "è¥¿å—": 5, "è¥¿åŒ—": 6
            }
            
            region_array = np.zeros((height, width), dtype=np.uint8)
            
            sample_step = 10
            
            print(f"      é‡‡æ ·æ­¥é•¿: {sample_step}")
            print(f"      è¯´æ˜: åŸºäºåŸå§‹æ•°æ®æ©è†œï¼Œä¸ä¼šé—æ¼ä»»ä½•æœ‰æ•ˆåƒå…ƒ")
            
            for i in tqdm(range(0, height, sample_step), desc="      ç”ŸæˆåŒºåŸŸæ …æ ¼"):
                for j in range(0, width, sample_step):
                    i_end = min(i + sample_step, height)
                    j_end = min(j + sample_step, width)
                    
                    block_valid = valid_mask[i:i_end, j:j_end]
                    
                    if not block_valid.any():
                        continue
                    
                    lon, lat = transform * (j + 0.5, i + 0.5)
                    macro_region = self.get_region(lon, lat)
                    
                    if macro_region:
                        code = region_codes[macro_region]
                        
                        temp_block = np.zeros((i_end - i, j_end - j), dtype=np.uint8)
                        temp_block[block_valid] = code
                        
                        region_array[i:i_end, j:j_end] = np.where(
                            block_valid,
                            temp_block,
                            region_array[i:i_end, j:j_end]
                        )
            
            print(f"      âœ… åŒºåŸŸæ …æ ¼ç”Ÿæˆå®Œæˆ")
            
            print(f"\n      ğŸ“Š å„åŒºåŸŸæœ‰æ•ˆåƒå…ƒç»Ÿè®¡:")
            total_assigned = 0
            for region, code in region_codes.items():
                count = ((region_array == code) & valid_mask).sum()
                percent = count / valid_count * 100 if valid_count > 0 else 0
                print(f"         {region}: {count:,} åƒå…ƒ ({percent:.1f}%)")
                total_assigned += count
            
            unassigned_valid = valid_count - total_assigned
            if unassigned_valid > 0:
                percent = unassigned_valid / valid_count * 100
                print(f"         æœªåˆ†é…: {unassigned_valid:,} åƒå…ƒ ({percent:.1f}%)")
            
            if output_path:
                meta = src.meta.copy()
                meta.update(count=1, dtype='uint8', compress='lzw', nodata=0)
                
                with rasterio.open(output_path, 'w', **meta) as dst:
                    dst.write(region_array, 1)
                
                print(f"      ğŸ’¾ åŒºåŸŸæ …æ ¼å·²ä¿å­˜: {output_path}")
            
            return region_array


# %% ============================================================================
#                           ç¬¬äº”éƒ¨åˆ†ï¼šé¢„ç­›é€‰ï¼ˆå¿«é€Ÿç‰ˆï¼‰
# ==============================================================================

def prefilter_valid_windows(day_tifs, windows, cache_dir):
    """
    é¢„ç­›é€‰æœ‰æ•ˆçª—å£ï¼ˆåŸå§‹åˆ†è¾¨ç‡ï¼Œ100%å‡†ç¡®ï¼‰
    
    åªç­›é€‰çª—å£ï¼Œä¸åŠ è½½æ•°æ®
    """
    cache_path = os.path.join(cache_dir, "valid_windows_cache.npz")
    
    if os.path.exists(cache_path):
        print(f"\nğŸ” åŠ è½½æœ‰æ•ˆçª—å£ç¼“å­˜...")
        data = np.load(cache_path)
        valid_windows = [tuple(w) for w in data['windows']]
        print(f"   âœ… åŠ è½½ {len(valid_windows):,} ä¸ªæœ‰æ•ˆçª—å£")
        return valid_windows
    
    print("\nğŸ” ç²¾ç¡®é¢„ç­›é€‰æœ‰æ•ˆçª—å£ï¼ˆåŸå§‹åˆ†è¾¨ç‡ï¼‰...")
    
    with rasterio.open(day_tifs[0]) as src:
        height = src.height
        width = src.width
        
        print(f"   å½±åƒå°ºå¯¸: {height:,} Ã— {width:,}")
        print(f"   è¯»å–æ•°æ®æ©è†œ...")
        
        data = src.read(1, masked=True)
        valid_mask = ~data.mask
        
        valid_percent = valid_mask.sum() / valid_mask.size * 100
        print(f"   æœ‰æ•ˆæ¯”ä¾‹: {valid_percent:.1f}%")
    
    valid_windows = []
    print(f"   ç­›é€‰çª—å£...")
    
    for row, col, win_h, win_w in tqdm(windows, desc="   ç­›é€‰è¿›åº¦"):
        window_mask = valid_mask[row:row+win_h, col:col+win_w]
        
        if window_mask.any():
            valid_windows.append((row, col, win_h, win_w))
    
    print(f"   âœ… ç­›é€‰å®Œæˆ: {len(valid_windows):,} / {len(windows):,}")
    
    filtered_ratio = (1 - len(valid_windows) / len(windows)) * 100
    print(f"   è¿‡æ»¤æ¯”ä¾‹: {filtered_ratio:.1f}%")
    
    # ä¿å­˜ç¼“å­˜
    np.savez_compressed(cache_path, windows=np.array(valid_windows, dtype=np.int32))
    print(f"   ğŸ’¾ ç¼“å­˜å·²ä¿å­˜")
    
    return valid_windows


# %% ============================================================================
#                           ç¬¬å…­éƒ¨åˆ†ï¼šDataLoaderæ•°æ®é›†
# ==============================================================================

class InferenceDataset(Dataset):
    """æ¨ç†æ•°æ®é›†ï¼ˆä»ç£ç›˜è¯»å–ï¼‰"""
    def __init__(self, windows, day_tifs, night_tif, region_array, patch_size=64):
        self.windows = windows
        self.day_tifs = day_tifs
        self.night_tif = night_tif
        self.region_array = region_array
        self.patch_size = patch_size
        
        # å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆåœ¨workerè¿›ç¨‹ä¸­ï¼‰
        self.day_srcs = None
        self.night_src = None
    
    def _init_sources(self):
        """åœ¨workerè¿›ç¨‹ä¸­åˆå§‹åŒ–æ–‡ä»¶å¥æŸ„"""
        if self.day_srcs is None:
            self.day_srcs = [rasterio.open(p) for p in self.day_tifs]
            self.night_src = rasterio.open(self.night_tif)
    
    def __len__(self):
        return len(self.windows)
    
    def __getitem__(self, idx):
        self._init_sources()
        
        row, col, win_h, win_w = self.windows[idx]
        window = Window(col, row, win_w, win_h)
        ps = self.patch_size
        
        # è¯»å–ç¬¬ä¸€ä¸ªæ³¢æ®µæ£€æŸ¥æœ‰æ•ˆæ€§
        arr0 = self.day_srcs[0].read(1, window=window, boundless=True, masked=True)
        
        if arr0.mask.all():
            return {
                'day': torch.zeros((len(self.day_tifs), ps, ps), dtype=torch.float32),
                'night': torch.zeros((1, ps, ps), dtype=torch.float32),
                'valid_mask': torch.zeros((ps, ps), dtype=torch.float32),
                'region_code': torch.tensor(0, dtype=torch.uint8),
                'meta': (row, col, 0, 0),
                'is_valid': False
            }
        
        valid_mask = (~arr0.mask).astype(np.float32)
        day_stack = [arr0.filled(0).astype(np.float32)]
        
        # è¯»å–å…¶ä»–æ³¢æ®µ
        for src in self.day_srcs[1:]:
            arr = src.read(1, window=window, boundless=True, masked=True)
            day_stack.append(arr.filled(0).astype(np.float32))
        
        night_arr = self.night_src.read(1, window=window, boundless=True, masked=True)
        night_arr = night_arr.filled(0).astype(np.float32)
        
        # Padding
        if win_h != ps or win_w != ps:
            pad_day = np.zeros((len(day_stack), ps, ps), dtype=np.float32)
            pad_night = np.zeros((1, ps, ps), dtype=np.float32)
            pad_valid = np.zeros((ps, ps), dtype=np.float32)
            
            pad_day[:, :win_h, :win_w] = np.stack(day_stack, axis=0)
            pad_night[:, :win_h, :win_w] = night_arr[np.newaxis, :, :]
            pad_valid[:win_h, :win_w] = valid_mask
            
            day_arr = pad_day
            night_arr = pad_night
            valid_mask = pad_valid
        else:
            day_arr = np.stack(day_stack, axis=0)
            night_arr = night_arr[np.newaxis, :, :]
        
        # è·å–åŒºåŸŸç¼–ç 
        center_row = row + win_h // 2
        center_col = col + win_w // 2
        region_code = self.region_array[center_row, center_col]
        
        return {
            'day': torch.from_numpy(day_arr),
            'night': torch.from_numpy(night_arr),
            'valid_mask': torch.from_numpy(valid_mask),
            'region_code': torch.tensor(region_code, dtype=torch.uint8),
            'meta': (row, col, win_h, win_w),
            'is_valid': True
        }


# %% ============================================================================
#                           ç¬¬ä¸ƒéƒ¨åˆ†ï¼šå·¥å…·å‡½æ•°
# ==============================================================================

def create_weight_patch(patch_size):
    """åˆ›å»ºæƒé‡çŸ©é˜µ"""
    weight = np.ones((patch_size, patch_size), dtype=np.float32)
    
    for i in range(patch_size):
        for j in range(patch_size):
            dist_i = min(i, patch_size - 1 - i) / (patch_size / 2)
            dist_j = min(j, patch_size - 1 - j) / (patch_size / 2)
            weight[i, j] = min(dist_i, dist_j)
    
    return weight


class CheckpointManager:
    """æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    def __init__(self, checkpoint_dir):
        self.checkpoint_dir = checkpoint_dir
        self.checkpoint_path = os.path.join(checkpoint_dir, "checkpoint_ensemble.npz")
        self.meta_path = os.path.join(checkpoint_dir, "checkpoint_ensemble_meta.json")
    
    def save(self, sum_comp, sum_weight, progress, total):
        try:
            np.savez_compressed(
                self.checkpoint_path,
                sum_comp=sum_comp,
                sum_weight=sum_weight
            )
            
            meta = {
                'progress': int(progress),
                'total': int(total),
                'progress_percent': progress / total * 100,
                'timestamp': time.time()
            }
            
            with open(self.meta_path, 'w') as f:
                json.dump(meta, f, indent=2)
            
            return True
        except Exception as e:
            print(f"\n   âš ï¸ æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def load(self):
        print(f"\nğŸ” æ£€æŸ¥æ£€æŸ¥ç‚¹...")
        
        if not os.path.exists(self.checkpoint_path):
            print("   âŒ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨ï¼Œä»å¤´å¼€å§‹")
            return None, None, 0
        
        file_size_gb = os.path.getsize(self.checkpoint_path) / 1e9
        print(f"   âœ… æ£€æŸ¥ç‚¹å­˜åœ¨ ({file_size_gb:.2f} GB)")
        
        try:
            print("   â³ åŠ è½½æ£€æŸ¥ç‚¹...")
            load_start = time.time()
            
            data = np.load(self.checkpoint_path)
            sum_comp = data['sum_comp']
            sum_weight = data['sum_weight']
            
            load_time = time.time() - load_start
            print(f"   âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ (è€—æ—¶ {load_time:.1f}ç§’)")
            
            if os.path.exists(self.meta_path):
                with open(self.meta_path, 'r') as f:
                    meta = json.load(f)
                progress = meta['progress']
                
                print(f"   ğŸ“Š ç»§ç»­æ¨ç†: {progress:,} / {meta['total']:,} ({meta['progress_percent']:.1f}%)")
            else:
                progress = 0
            
            return sum_comp, sum_weight, progress
        
        except Exception as e:
            print(f"\n   âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            return None, None, 0
    
    def clean(self):
        try:
            if os.path.exists(self.checkpoint_path):
                os.remove(self.checkpoint_path)
            if os.path.exists(self.meta_path):
                os.remove(self.meta_path)
            return True
        except:
            return False


# %% ============================================================================
#                           ç¬¬å…«éƒ¨åˆ†ï¼šä¸»æ¨ç†å‡½æ•°
# ==============================================================================

def infer_full_raster_ensemble(random_model_path, oor_models_dict, province_shp):
    """å…¨å›¾æ¨ç†å‡½æ•°ï¼ˆåŒºåŸŸè‡ªé€‚åº”é›†æˆ+å†…å­˜å®‰å…¨ç‰ˆï¼‰"""
    
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹å…¨å›¾æ¨ç†ï¼ˆåŒºåŸŸè‡ªé€‚åº”é›†æˆ+å†…å­˜å®‰å…¨ï¼‰")
    print("=" * 80)
    
    start_time = time.time()
    
    # 1. åˆå§‹åŒ–åŒºåŸŸåŒ¹é…ç³»ç»Ÿ
    region_matcher = RegionMatcher(province_shp)
    
    # 2. è·å–å½±åƒä¿¡æ¯
    print("\nğŸ“‚ è¯»å–å½±åƒä¿¡æ¯...")
    with rasterio.open(DAY_TIFS[0]) as src:
        height = src.height
        width = src.width
        transform = src.transform
        crs = src.crs
        meta = src.meta.copy()
    
    print(f"   å½±åƒå°ºå¯¸: {height:,} Ã— {width:,} åƒç´ ")
    
    # 3. ç”ŸæˆåŒºåŸŸæ …æ ¼
    region_raster_path = os.path.join(OUT_DIR, "region_codes.tif")
    region_array = region_matcher.create_region_raster(DAY_TIFS[0], region_raster_path)
    
    # 4. ç”Ÿæˆæ‰€æœ‰çª—å£
    print("\nğŸ”„ ç”Ÿæˆæ¨ç†çª—å£...")
    all_windows = []
    for row in range(0, height, STEP):
        for col in range(0, width, STEP):
            win_h = min(PATCH_SIZE, height - row)
            win_w = min(PATCH_SIZE, width - col)
            all_windows.append((row, col, win_h, win_w))
    
    print(f"   åŸå§‹çª—å£: {len(all_windows):,}")
    
    # 5. é¢„ç­›é€‰æœ‰æ•ˆçª—å£
    valid_windows = prefilter_valid_windows(DAY_TIFS, all_windows, OUT_DIR)
    
    total_valid_windows = len(valid_windows)
    print(f"\n   âœ… æœ‰æ•ˆçª—å£: {total_valid_windows:,}")
    
    # 6. åŠ è½½æ‰€æœ‰æ¨¡å‹
    print("\nğŸ”§ åŠ è½½æ¨¡å‹...")
    print("   [1/7] Randomä¸»æ¨¡å‹...")
    
    model_random = FusionResNetDirichlet(
        day_channels=len(DAY_TIFS),
        night_channels=1,
        n_comp=len(TARGET_FIELDS),
        base_channels=64,
        day_blocks=3,
        night_blocks=3,
        shared_blocks=5,
    )
    
    try:
        state = torch.load(random_model_path, map_location='cpu')
        model_random.load_state_dict(state)
        model_random = model_random.cuda()
        model_random.eval()
        print("      âœ… Randomæ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"      âŒ Randomæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)
    
    # åŠ è½½6ä¸ªOORæ¨¡å‹
    models_oor = {}
    region_codes_map = {
        "ä¸œåŒ—": 1, "ååŒ—": 2, "åä¸œ": 3,
        "ä¸­å—": 4, "è¥¿å—": 5, "è¥¿åŒ—": 6
    }
    
    for i, (region, model_path) in enumerate(oor_models_dict.items(), 2):
        print(f"   [{i}/7] OORæ¨¡å‹ - {region}...")
        
        model = FusionResNetDirichlet(
            day_channels=len(DAY_TIFS),
            night_channels=1,
            n_comp=len(TARGET_FIELDS),
            base_channels=64,
            day_blocks=3,
            night_blocks=3,
            shared_blocks=5,
        )
        
        try:
            state = torch.load(model_path, map_location='cpu')
            model.load_state_dict(state)
            model = model.cuda()
            model.eval()
            
            code = region_codes_map[region]
            models_oor[code] = model
            
            print(f"      âœ… {region}æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"      âŒ {region}æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    
    print("\n   âœ… å…¨éƒ¨æ¨¡å‹åŠ è½½å®Œæˆ (7ä¸ª)")
    
    # 7. æ˜¾ç¤ºæƒé‡é…ç½®
    print("\nğŸ“Š åŒºåŸŸè‡ªé€‚åº”æƒé‡é…ç½®:")
    for region, weights in REGION_WEIGHTS.items():
        print(f"   {region}: Random {weights['random']*100:.0f}%, OOR {weights['oor']*100:.0f}%")
    
    # 8. åˆå§‹åŒ–ç´¯ç§¯æ•°ç»„
    print("\nğŸ’¾ åˆ†é…ç´¯ç§¯æ•°ç»„...")
    n_comp = len(TARGET_FIELDS)
    
    try:
        sum_comp = np.zeros((n_comp, height, width), dtype=np.float32)
        sum_weight = np.zeros((height, width), dtype=np.float32)
    except MemoryError:
        print("âŒ å†…å­˜ä¸è¶³ï¼")
        sys.exit(1)
    
    memory_gb = (sum_comp.nbytes + sum_weight.nbytes) / 1e9
    print(f"   å·²åˆ†é…: {memory_gb:.2f} GB")
    
    # 9. å‡†å¤‡æƒé‡çŸ©é˜µ
    weight_patch = create_weight_patch(PATCH_SIZE)
    
    # 10. æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    checkpoint_mgr = CheckpointManager(OUT_DIR)
    loaded_comp, loaded_weight, start_idx = checkpoint_mgr.load()
    
    if loaded_comp is not None:
        sum_comp[:] = loaded_comp
        sum_weight[:] = loaded_weight
    else:
        start_idx = 0
    
    # 11. åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
    print("\nâš™ï¸ å‡†å¤‡æ•°æ®åŠ è½½å™¨ï¼ˆä»ç£ç›˜å¤šè¿›ç¨‹è¯»å–ï¼‰...")
    dataset = InferenceDataset(valid_windows, DAY_TIFS, NIGHT_TIF, region_array, PATCH_SIZE)
    
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=NUM_WORKERS,
        prefetch_factor=PREFETCH_FACTOR if NUM_WORKERS > 0 else None,
        pin_memory=PIN_MEMORY,
        persistent_workers=False  # num_workers=0æ—¶å¿…é¡»ä¸ºFalse
    )
    
    # 12. æ¨ç†å¾ªç¯ï¼ˆé›†æˆï¼‰
    print("\nâ³ å¼€å§‹æ¨ç†ï¼ˆåŒºåŸŸè‡ªé€‚åº”é›†æˆï¼‰...")
    processed_windows = start_idx
    start_batch = start_idx // BATCH_SIZE
    
    region_stats = {i: 0 for i in range(7)}
    
    with torch.no_grad():
        pbar = tqdm(
            enumerate(dataloader),
            total=len(dataloader),
            desc="æ¨ç†è¿›åº¦",
            initial=start_batch
        )
        
        for batch_idx, batch in pbar:
            if batch_idx < start_batch:
                continue
            
            valid_indices = batch['is_valid']
            if not valid_indices.any():
                continue
            
            day_data = batch['day'][valid_indices].cuda(non_blocking=True)
            night_data = batch['night'][valid_indices].cuda(non_blocking=True)
            region_codes_batch = batch['region_code'][valid_indices]
            
            # Randomæ¨¡å‹é¢„æµ‹
            if USE_AMP:
                with autocast():
                    alpha_random = model_random(day_data, night_data)
            else:
                alpha_random = model_random(day_data, night_data)
            
            alpha_random = alpha_random.cpu().numpy()
            
            # åŒºåŸŸè‡ªé€‚åº”é›†æˆ
            batch_size_actual = alpha_random.shape[0]
            alpha_ensemble = np.zeros_like(alpha_random)
            
            for i in range(batch_size_actual):
                region_code = int(region_codes_batch[i])
                region_stats[region_code] += 1
                
                # è·å–è¯¥åŒºåŸŸçš„æƒé‡
                region_name = None
                for name, code in region_codes_map.items():
                    if code == region_code:
                        region_name = name
                        break
                
                if region_name and region_name in REGION_WEIGHTS:
                    w_random = REGION_WEIGHTS[region_name]['random']
                    w_oor = REGION_WEIGHTS[region_name]['oor']
                    
                    alpha_ensemble[i] = w_random * alpha_random[i]
                    
                    if w_oor > 0 and region_code in models_oor:
                        day_single = day_data[i:i+1]
                        night_single = night_data[i:i+1]
                        
                        if USE_AMP:
                            with autocast():
                                alpha_oor = models_oor[region_code](day_single, night_single)
                        else:
                            alpha_oor = models_oor[region_code](day_single, night_single)
                        
                        alpha_oor = alpha_oor.cpu().numpy()
                        alpha_ensemble[i] += w_oor * alpha_oor[0]
                else:
                    alpha_ensemble[i] = alpha_random[i]
            
            # å†™å›ç»“æœ
            valid_idx = 0
            batch_processed = 0
            
            for i, is_valid in enumerate(valid_indices):
                if not is_valid:
                    continue
                
                row = int(batch['meta'][0][i])
                col = int(batch['meta'][1][i])
                win_h = int(batch['meta'][2][i])
                win_w = int(batch['meta'][3][i])
                
                if win_h == 0:
                    continue
                
                valid_mask = batch['valid_mask'][i].numpy()
                comp = alpha_ensemble[valid_idx]
                comp = comp / np.clip(comp.sum(axis=0, keepdims=True), 1e-6, None)
                
                w_full = weight_patch * valid_mask
                w = w_full[:win_h, :win_w]
                
                sum_comp[:, row:row+win_h, col:col+win_w] += comp[:, :win_h, :win_w] * w
                sum_weight[row:row+win_h, col:col+win_w] += w
                
                valid_idx += 1
                batch_processed += 1
            
            processed_windows += batch_processed
            
            pbar.set_postfix({
                'processed': f'{processed_windows:,}/{total_valid_windows:,}',
                'GPU': f'{torch.cuda.memory_allocated()/1e9:.1f}GB'
            })
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if AUTO_SAVE and batch_idx % CHECKPOINT_INTERVAL == 0 and batch_idx > start_batch:
                checkpoint_mgr.save(sum_comp, sum_weight, processed_windows, total_valid_windows)
    
    elapsed_time = time.time() - start_time
    print(f"\nâœ… æ¨ç†å®Œæˆ! è€—æ—¶: {elapsed_time/3600:.2f} å°æ—¶")
    
    # æ˜¾ç¤ºåŒºåŸŸç»Ÿè®¡
    print("\nğŸ“Š å„åŒºåŸŸçª—å£ç»Ÿè®¡:")
    region_names_map = {0: "æœªçŸ¥", 1: "ä¸œåŒ—", 2: "ååŒ—", 3: "åä¸œ", 4: "ä¸­å—", 5: "è¥¿å—", 6: "è¥¿åŒ—"}
    for code, count in region_stats.items():
        name = region_names_map[code]
        percent = count / processed_windows * 100 if processed_windows > 0 else 0
        print(f"   {name}: {count:,} ({percent:.1f}%)")
    
    # ========== æ¨ç†å®Œæˆåç«‹å³æ¸…ç†å†…å­˜ ========== 
    print("\nğŸ§¹ æ¸…ç†æ¨ç†èµ„æºï¼ˆä¸ºå½’ä¸€åŒ–é‡Šæ”¾å†…å­˜ï¼‰...")
    
    print("   [1/5] åˆ é™¤DataLoader...")
    del dataloader
    
    print("   [2/5] åˆ é™¤Dataset...")
    del dataset
    
    print("   [3/5] åˆ é™¤æ‰€æœ‰æ¨¡å‹...")
    del model_random
    del models_oor
    
    print("   [4/5] æ¸…ç©ºGPUç¼“å­˜...")
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print("   [5/5] å¼ºåˆ¶åƒåœ¾å›æ”¶...")
    gc.collect()
    
    # æ˜¾ç¤ºé‡Šæ”¾åçš„å†…å­˜çŠ¶æ€
    try:
        import psutil
        mem = psutil.virtual_memory()
        print(f"\n   âœ… å†…å­˜é‡Šæ”¾å®Œæˆ:")
        print(f"      å¯ç”¨å†…å­˜: {mem.available / 1e9:.1f} GB / {mem.total / 1e9:.1f} GB")
    except:
        pass
    
    print(f"\n   ğŸ’¡ åªä¿ç•™ç´¯ç§¯æ•°ç»„ ({memory_gb:.2f} GB)")
    
    # ========== å½’ä¸€åŒ–å¹¶å†™å‡ºï¼ˆåˆ†å—å¤„ç†ï¼‰ ==========
    print(f"\nğŸ“Š å½’ä¸€åŒ–å¹¶å†™å‡ºç»“æœï¼ˆåˆ†å—å¤„ç†ï¼‰...")
    
    meta.update(count=1, dtype='float32', compress='lzw', nodata=-9999)
    output_files = []
    
    CHUNK_ROWS = 5000
    
    for k, name in enumerate(TARGET_FIELDS):
        print(f"\n   [{k+1}/{len(TARGET_FIELDS)}] å¤„ç† {name}")
        
        fname = f"pred_ensemble_adaptive_{name}_2020_90m.tif"
        out_path = os.path.join(OUT_DIR, fname)
        
        print(f"      åˆ†å—å½’ä¸€åŒ–å¹¶å†™å‡º...")
        
        valid_count = 0
        sum_values = 0.0
        
        with rasterio.open(out_path, 'w', **meta) as dst:
            num_chunks = (height + CHUNK_ROWS - 1) // CHUNK_ROWS
            
            for i in tqdm(range(num_chunks), desc=f"      {name}", leave=False):
                start_row = i * CHUNK_ROWS
                end_row = min(start_row + CHUNK_ROWS, height)
                
                comp_chunk = sum_comp[k, start_row:end_row, :]
                weight_chunk = sum_weight[start_row:end_row, :]
                
                weight_safe = np.where(weight_chunk > 0, weight_chunk, 1.0)
                normalized_chunk = comp_chunk / weight_safe
                normalized_chunk[weight_chunk == 0] = -9999
                
                valid_mask = (weight_chunk > 0)
                if valid_mask.any():
                    valid_values = normalized_chunk[valid_mask]
                    valid_count += len(valid_values)
                    sum_values += valid_values.sum()
                
                dst.write(normalized_chunk, 1, window=Window(0, start_row, width, end_row - start_row))
                
                del weight_safe, normalized_chunk
                
                if i % 10 == 0:
                    gc.collect()
        
        if valid_count > 0:
            mean = sum_values / valid_count
            print(f"      ç»Ÿè®¡: Î¼={mean:.4f}, N={valid_count:,}")
        
        file_size = os.path.getsize(out_path) / 1e6
        print(f"      âœ… å®Œæˆ ({file_size:.1f} MB)")
        
        output_files.append(out_path)
        
        gc.collect()
    
    # æœ€ç»ˆæ¸…ç†
    print("\nğŸ§¹ æœ€ç»ˆæ¸…ç†...")
    del sum_comp, sum_weight
    gc.collect()
    
    print("\n" + "=" * 80)
    print("ğŸ‰ åŒºåŸŸè‡ªé€‚åº”é›†æˆæ¨ç†å®Œæˆï¼")
    print("=" * 80)
    
    return output_files


# %% ============================================================================
#                           ç¬¬ä¹éƒ¨åˆ†ï¼šä¸»ç¨‹åº
# ==============================================================================

def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "=" * 80)
    print("å¼€å§‹æ‰§è¡Œå…¨å›¾æ¨ç†ï¼ˆåŒºåŸŸè‡ªé€‚åº”é›†æˆ+å†…å­˜å®‰å…¨ï¼‰")
    print("=" * 80)
    
    # æ‰§è¡Œæ¨ç†
    output_files = infer_full_raster_ensemble(
        random_model_path=RANDOM_MODEL_PATH,
        oor_models_dict=OOR_MODELS,
        province_shp=PROVINCE_SHP
    )
    
    # è¾“å‡ºæ€»ç»“
    print("\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    for f in output_files:
        print(f"   {f}")
    
    print("\nğŸ’¡ ä¼˜åŒ–æ€»ç»“:")
    print("   âœ… é¢„ç­›é€‰: åŸå§‹åˆ†è¾¨ç‡ï¼ˆ100%å‡†ç¡®ï¼‰")
    print("   âœ… æ•°æ®è¯»å–: DataLoaderå¤šè¿›ç¨‹ï¼ˆä¸é¢„åŠ è½½ï¼‰")
    print("   âœ… æ¨ç†åæ¸…ç†: é‡Šæ”¾æ¨¡å‹ï¼ˆ~12GBï¼‰")
    print("   âœ… åˆ†å—å½’ä¸€åŒ–: é¿å…OOMï¼ˆå³°å€¼~3GBï¼‰")
    print("   âœ… NoDataå¤„ç†: åªå¯¹æœ‰æ•ˆåƒå…ƒé¢„æµ‹")
    print("   âœ… åŒºåŸŸè‡ªé€‚åº”: æ™ºèƒ½æƒé‡é›†æˆ")
    print("\n   å†…å­˜å³°å€¼: ~15GBï¼ˆå®‰å…¨ï¼‰")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ¨ç†ï¼ˆæ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼‰")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)